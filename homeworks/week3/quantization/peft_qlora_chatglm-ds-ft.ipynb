{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb393a61",
   "metadata": {},
   "source": [
    "# peft qlora chatglm with different dataset\n",
    "\n",
    "```\n",
    "Á¨¨‰∏âÂë®‰Ωú‰∏ö‰∏Ä: \n",
    "1„ÄÅ‰ΩøÁî® GPTQ ÈáèÂåñ OPT-6.7B Ê®°Âûã„ÄÇËØæÁ®ã‰ª£Á†ÅÔºà https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AutoGPTQ_opt-2.7b.ipynb Ôºâ\n",
    "2„ÄÅ‰ΩøÁî® AWQ ÈáèÂåñ Facebook OPT-6.7B Ê®°Âûã„ÄÇFacebook OPT Ê®°ÂûãÂú∞ÂùÄÔºö https://huggingface.co/facebook?search_models=opt\n",
    "ËØæÁ®ã‰ª£Á†ÅÔºö https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AWQ_opt-2.7b.ipynb\n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AWQ-opt-125m.ipynb \n",
    "\n",
    "Á¨¨‰∏âÂë®‰Ωú‰∏ö‰∫åÔºö Ê†πÊçÆÁ°¨‰ª∂ËµÑÊ∫êÊÉÖÂÜµÔºåÂú® AdvertiseGen Êï∞ÊçÆÈõÜ‰∏ä‰ΩøÁî® QLoRA ÂæÆË∞É ChatGLM3-6B Ëá≥Â∞ë 10K examplesÔºåËßÇÂØü Loss ÂèòÂåñÊÉÖÂÜµÔºåÂπ∂ÂØπÊØîÂæÆË∞ÉÂâçÂêéÊ®°ÂûãËæìÂá∫ÁªìÊûú„ÄÇ------> this notebook is for this task.\n",
    "ËØæÁ®ã‰ª£Á†ÅÔºö \n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_qlora_chatglm.ipynb\n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_chatglm_inference.ipynb\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa8105c-6dda-426b-9180-ab9abbc9ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üîç Train/Eval with dataset: HasturOfficial/adgen\n",
      "==================================================\n",
      "num_train_samples: 1000\n",
      "num_eval_samples: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d20e890a3948fa8912efe58ca399de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 974,848 || all params: 6,244,558,848 || trainable%: 0.01561115883009451\n",
      "training_args num_train_epochs: 1, logging_steps: 10, eval_steps: 10, save_steps: 30, per_device_train_batch_size: 8, , gradient_accumulation_steps: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 52:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.274100</td>\n",
       "      <td>3.904693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.691600</td>\n",
       "      <td>3.761558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.599500</td>\n",
       "      <td>3.688349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.586000</td>\n",
       "      <td>3.647342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.503300</td>\n",
       "      <td>3.624843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.532200</td>\n",
       "      <td>3.612717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/finetuned_HasturOfficial_adgen/checkpoint-30 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ fine tuned model saved to path: models/finetuned_HasturOfficial_adgen\n",
      "\n",
      "==================================================\n",
      "üîç Train/Eval with dataset: shibing624/AdvertiseGen\n",
      "==================================================\n",
      "num_train_samples: 1000\n",
      "num_eval_samples: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 142\u001b[39m\n\u001b[32m    137\u001b[39m     tokenized_eval = eval_dataset_loaded.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: tokenize_func(x, tokenizer), batched=\u001b[38;5;28;01mFalse\u001b[39;00m, remove_columns=column_names_eval)\n\u001b[32m    139\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;66;03m# 4. Ê®°Âûã„ÄÅLoRA„ÄÅQLoRA ÂàùÂßãÂåñ\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBitsAndBytesConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnf4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_compute_dtype_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mb098244\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m    153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m model = prepare_model_for_kbit_training(model)\n\u001b[32m    155\u001b[39m target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.get(\u001b[33m'\u001b[39m\u001b[33mchatglm\u001b[39m\u001b[33m'\u001b[39m, [\u001b[33m'\u001b[39m\u001b[33mquery_key_value\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:561\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    560\u001b[39m         \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    565\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/transformers/modeling_utils.py:3790\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3786\u001b[39m         device_map_without_lm_head = {\n\u001b[32m   3787\u001b[39m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map.keys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[32m   3788\u001b[39m         }\n\u001b[32m   3789\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m-> \u001b[39m\u001b[32m3790\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3791\u001b[39m \u001b[38;5;250m                \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3792\u001b[39m \u001b[33;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[32m   3793\u001b[39m \u001b[33;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[32m   3794\u001b[39m \u001b[33;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[32m   3795\u001b[39m \u001b[33;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[32m   3796\u001b[39m \u001b[33;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[32m   3797\u001b[39m \u001b[33;03m                for more details.\u001b[39;00m\n\u001b[32m   3798\u001b[39m \u001b[33;03m                \"\"\"\u001b[39;00m\n\u001b[32m   3799\u001b[39m             )\n\u001b[32m   3800\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[32m   3802\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "# this cell is to train the model with dataset: HasturOfficial/adgen, seperated to a new cell for GPU mem freeup\n",
    "from datasets import load_dataset, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from peft import TaskType, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ======================\n",
    "# 1. parameneters setup\n",
    "# ======================\n",
    "\n",
    "model_name_or_path = 'THUDM/chatglm3-6b'\n",
    "seed = 8\n",
    "max_input_length = 512\n",
    "max_output_length = 1536\n",
    "lora_rank = 4\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "resume_from_checkpoint = None\n",
    "prompt_text = ''\n",
    "compute_dtype = 'bf16'  # fp32 / fp16 / bf16\n",
    "\n",
    "# support multiple dataset_names to be tested\n",
    "dataset_names = [\n",
    "    \"HasturOfficial/adgen\",\n",
    "    #\"shibing624/AdvertiseGen\",\n",
    "    #can add more dataset for test\n",
    "]\n",
    "\n",
    "# not use 10k data as it takes too much time...\n",
    "num_train_samples = 1000\n",
    "# use 0.1 of train data\n",
    "num_eval_samples = num_train_samples // 10\n",
    "\n",
    "# training paramenters setup\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "learning_rate = 1e-3\n",
    "num_train_epochs = 1\n",
    "#eval_steps = num_train_samples // (5 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "#save_steps = num_train_samples // (3 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "#logging_steps = num_train_samples // (15 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "logging_steps = 10\n",
    "eval_steps = 10\n",
    "save_steps = 30\n",
    "\n",
    "# dtype mapping\n",
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# the tokenize function\n",
    "def tokenize_func(example, tokenizer, ignore_label_id=-100):\n",
    "    question = prompt_text + example['content']\n",
    "    if example.get('input', None) and example['input'].strip():\n",
    "        question += f'\\n{example[\"input\"]}'\n",
    "    answer = example['summary']\n",
    "    q_ids = tokenizer.encode(text=question, add_special_tokens=False)\n",
    "    a_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
    "    if len(q_ids) > max_input_length - 2:\n",
    "        q_ids = q_ids[:max_input_length - 2]\n",
    "    if len(a_ids) > max_output_length - 1:\n",
    "        a_ids = a_ids[:max_output_length - 1]\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(q_ids, a_ids)\n",
    "    question_length = len(q_ids) + 2\n",
    "    labels = [ignore_label_id] * question_length + input_ids[question_length:]\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "# the DataCollator class\n",
    "class DataCollatorForChatGLM:\n",
    "    def __init__(self, pad_token_id: int, max_length: int = 2048, ignore_label_id: int = -100):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.ignore_label_id = ignore_label_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        len_list = [len(d['input_ids']) for d in batch]\n",
    "        batch_max_len = max(len_list)\n",
    "\n",
    "        input_ids, labels = [], []\n",
    "        for len_of_d, d in sorted(zip(len_list, batch), key=lambda x: -x[0]):\n",
    "            pad_len = batch_max_len - len_of_d\n",
    "            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n",
    "            label = d['labels'] + [self.ignore_label_id] * pad_len\n",
    "            if batch_max_len > self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                label = label[:self.max_length]\n",
    "            input_ids.append(torch.LongTensor(ids))\n",
    "            labels.append(torch.LongTensor(label))\n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids),\n",
    "            'labels': torch.stack(labels)\n",
    "        }\n",
    "        \n",
    "# tain/eval for the datasets\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîç Train/Eval with dataset: {dataset_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # ======================\n",
    "    # 1. load dataset\n",
    "    # ======================\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    eval_dataset_loaded = None\n",
    "    if 'validation' in dataset:\n",
    "        eval_dataset_loaded = dataset['validation']\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  The dataset doesn't have 'validation' splitÔºå not evaluating.\")\n",
    "\n",
    "    # ======================\n",
    "    # 2. set the used train/eval samples\n",
    "    # ======================\n",
    "    if num_train_samples is not None and num_train_samples > 0:\n",
    "        print(f\"num_train_samples: {num_train_samples}\")\n",
    "        dataset['train'] = dataset['train'].select(range(num_train_samples))\n",
    "    if eval_dataset_loaded is not None and num_eval_samples is not None and num_eval_samples > 0:\n",
    "        print(f\"num_eval_samples: {num_eval_samples}\")\n",
    "        eval_dataset_loaded = eval_dataset_loaded.select(range(num_eval_samples))\n",
    "\n",
    "    # ======================\n",
    "    # 3. Tokenize\n",
    "    # ======================\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, revision='b098244')\n",
    "\n",
    "    column_names = dataset['train'].column_names\n",
    "    tokenized_train = dataset['train'].map(lambda x: tokenize_func(x, tokenizer), batched=False, remove_columns=column_names)\n",
    "\n",
    "    tokenized_eval = None\n",
    "    if eval_dataset_loaded is not None:\n",
    "        column_names_eval = eval_dataset_loaded.column_names\n",
    "        tokenized_eval = eval_dataset_loaded.map(lambda x: tokenize_func(x, tokenizer), batched=False, remove_columns=column_names_eval)\n",
    "\n",
    "    # ======================\n",
    "    # 4. init the model, LoRA„ÄÅQLoRA\n",
    "    # ======================\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=_compute_dtype_map[compute_dtype]\n",
    "        ),\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        revision='b098244'\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.get('chatglm', ['query_key_value'])\n",
    "    lora_config = LoraConfig(\n",
    "        target_modules=target_modules,\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias='none',\n",
    "        inference_mode=False,\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    qlora_model = get_peft_model(model, lora_config)\n",
    "    qlora_model.print_trainable_parameters()\n",
    "\n",
    "    # ======================\n",
    "    # 5. TrainingArguments & Trainer\n",
    "    # ======================\n",
    "    saved_dir = f\"models/finetuned_{dataset_name.replace('/', '_')}\"\n",
    "    print(f\"training_args num_train_epochs: {num_train_epochs}, \\\n",
    "logging_steps: {logging_steps}, eval_steps: {eval_steps}, save_steps: {save_steps}\\\n",
    ", per_device_train_batch_size: {per_device_train_batch_size}, , gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=saved_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        evaluation_strategy=\"steps\" if eval_dataset_loaded is not None else \"no\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        fp16=(compute_dtype == 'fp16'),\n",
    "        bf16=(compute_dtype == 'bf16'),\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # 6. train & auto print the train/validation loss\n",
    "    # ======================\n",
    "    trainer.train()\n",
    "\n",
    "    # ======================\n",
    "    # 7. saved the tuned model\n",
    "    # ======================\n",
    "    trainer.model.save_pretrained(saved_dir)\n",
    "    print(f\"‚úÖ fine tuned model saved to path: {saved_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a700ca6-e5a2-4465-8ed6-8ca440108b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Releasing GPU ...\n",
      "\n",
      "==================================================\n",
      "üîç Train/Eval with dataset: shibing624/AdvertiseGen\n",
      "==================================================\n",
      "num_train_samples: 1000\n",
      "num_eval_samples: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b336d297ae5c46588e422097de6be97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 974,848 || all params: 6,244,558,848 || trainable%: 0.01561115883009451\n",
      "training_args num_train_epochs: 1, logging_steps: 10, eval_steps: 10, save_steps: 30, per_device_train_batch_size: 8, , gradient_accumulation_steps: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62/62 51:38, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.271100</td>\n",
       "      <td>3.901426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.688500</td>\n",
       "      <td>3.760577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.600400</td>\n",
       "      <td>3.694746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.588100</td>\n",
       "      <td>3.652654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.505300</td>\n",
       "      <td>3.631722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.539800</td>\n",
       "      <td>3.619895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ fine tuned model saved to path: models/finetuned_shibing624_AdvertiseGen\n"
     ]
    }
   ],
   "source": [
    "# this cell is to train the model with dataset: shibing624/AdvertiseGen, seperated to a new cell for GPU mem freeup\n",
    "# free up gpu mem before processing next here.\n",
    "def freeup_mem():\n",
    "    print(\"\\nüîß Releasing GPU ...\")\n",
    "    import torch, gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    %reset -f\n",
    "# call to freeup mem    \n",
    "freeup_mem()\n",
    "\n",
    "from datasets import load_dataset, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from peft import TaskType, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
    "from transformers import AutoModel, BitsAndBytesConfig\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# ======================\n",
    "# 1. parameneters setup\n",
    "# ======================\n",
    "\n",
    "model_name_or_path = 'THUDM/chatglm3-6b'\n",
    "seed = 8\n",
    "max_input_length = 512\n",
    "max_output_length = 1536\n",
    "lora_rank = 4\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "resume_from_checkpoint = None\n",
    "prompt_text = ''\n",
    "compute_dtype = 'bf16'  # fp32 / fp16 / bf16\n",
    "\n",
    "# support multiple dataset_names to be tested\n",
    "dataset_names = [\n",
    "    #\"HasturOfficial/adgen\",\n",
    "    \"shibing624/AdvertiseGen\",\n",
    "    #can add more dataset for test\n",
    "]\n",
    "\n",
    "# not use 10k data as it takes too much time...\n",
    "num_train_samples = 1000\n",
    "# use 0.1 of train data\n",
    "num_eval_samples = num_train_samples // 10\n",
    "\n",
    "# training paramenters setup\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "gradient_accumulation_steps = 2\n",
    "learning_rate = 1e-3\n",
    "num_train_epochs = 1\n",
    "#eval_steps = num_train_samples // (5 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "#save_steps = num_train_samples // (3 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "#logging_steps = num_train_samples // (15 * per_device_train_batch_size * gradient_accumulation_steps)\n",
    "logging_steps = 10\n",
    "eval_steps = 10\n",
    "save_steps = 30\n",
    "\n",
    "# dtype mapping\n",
    "_compute_dtype_map = {\n",
    "    'fp32': torch.float32,\n",
    "    'fp16': torch.float16,\n",
    "    'bf16': torch.bfloat16\n",
    "}\n",
    "\n",
    "# the tokenize function\n",
    "def tokenize_func(example, tokenizer, ignore_label_id=-100):\n",
    "    question = prompt_text + example['content']\n",
    "    if example.get('input', None) and example['input'].strip():\n",
    "        question += f'\\n{example[\"input\"]}'\n",
    "    answer = example['summary']\n",
    "    q_ids = tokenizer.encode(text=question, add_special_tokens=False)\n",
    "    a_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
    "    if len(q_ids) > max_input_length - 2:\n",
    "        q_ids = q_ids[:max_input_length - 2]\n",
    "    if len(a_ids) > max_output_length - 1:\n",
    "        a_ids = a_ids[:max_output_length - 1]\n",
    "    input_ids = tokenizer.build_inputs_with_special_tokens(q_ids, a_ids)\n",
    "    question_length = len(q_ids) + 2\n",
    "    labels = [ignore_label_id] * question_length + input_ids[question_length:]\n",
    "    return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "# the DataCollator class\n",
    "class DataCollatorForChatGLM:\n",
    "    def __init__(self, pad_token_id: int, max_length: int = 2048, ignore_label_id: int = -100):\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.ignore_label_id = ignore_label_id\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        len_list = [len(d['input_ids']) for d in batch]\n",
    "        batch_max_len = max(len_list)\n",
    "\n",
    "        input_ids, labels = [], []\n",
    "        for len_of_d, d in sorted(zip(len_list, batch), key=lambda x: -x[0]):\n",
    "            pad_len = batch_max_len - len_of_d\n",
    "            ids = d['input_ids'] + [self.pad_token_id] * pad_len\n",
    "            label = d['labels'] + [self.ignore_label_id] * pad_len\n",
    "            if batch_max_len > self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "                label = label[:self.max_length]\n",
    "            input_ids.append(torch.LongTensor(ids))\n",
    "            labels.append(torch.LongTensor(label))\n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids),\n",
    "            'labels': torch.stack(labels)\n",
    "        }\n",
    "        \n",
    "# tain/eval for the datasets\n",
    "for dataset_name in dataset_names:\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"üîç Train/Eval with dataset: {dataset_name}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # ======================\n",
    "    # 1. load dataset\n",
    "    # ======================\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    eval_dataset_loaded = None\n",
    "    if 'validation' in dataset:\n",
    "        eval_dataset_loaded = dataset['validation']\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  The dataset doesn't have 'validation' splitÔºå not evaluating.\")\n",
    "\n",
    "    # ======================\n",
    "    # 2. set the used train/eval samples\n",
    "    # ======================\n",
    "    if num_train_samples is not None and num_train_samples > 0:\n",
    "        print(f\"num_train_samples: {num_train_samples}\")\n",
    "        dataset['train'] = dataset['train'].select(range(num_train_samples))\n",
    "    if eval_dataset_loaded is not None and num_eval_samples is not None and num_eval_samples > 0:\n",
    "        print(f\"num_eval_samples: {num_eval_samples}\")\n",
    "        eval_dataset_loaded = eval_dataset_loaded.select(range(num_eval_samples))\n",
    "\n",
    "    # ======================\n",
    "    # 3. Tokenize\n",
    "    # ======================\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, revision='b098244')\n",
    "\n",
    "    column_names = dataset['train'].column_names\n",
    "    tokenized_train = dataset['train'].map(lambda x: tokenize_func(x, tokenizer), batched=False, remove_columns=column_names)\n",
    "\n",
    "    tokenized_eval = None\n",
    "    if eval_dataset_loaded is not None:\n",
    "        column_names_eval = eval_dataset_loaded.column_names\n",
    "        tokenized_eval = eval_dataset_loaded.map(lambda x: tokenize_func(x, tokenizer), batched=False, remove_columns=column_names_eval)\n",
    "\n",
    "    # ======================\n",
    "    # 4. init the model, LoRA„ÄÅQLoRA\n",
    "    # ======================\n",
    "    model = AutoModel.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        quantization_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type='nf4',\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=_compute_dtype_map[compute_dtype]\n",
    "        ),\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        revision='b098244'\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    target_modules = TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING.get('chatglm', ['query_key_value'])\n",
    "    lora_config = LoraConfig(\n",
    "        target_modules=target_modules,\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias='none',\n",
    "        inference_mode=False,\n",
    "        task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    qlora_model = get_peft_model(model, lora_config)\n",
    "    qlora_model.print_trainable_parameters()\n",
    "\n",
    "    # ======================\n",
    "    # 5. TrainingArguments & Trainer\n",
    "    # ======================\n",
    "    saved_dir = f\"models/finetuned_{dataset_name.replace('/', '_')}\"\n",
    "    print(f\"training_args num_train_epochs: {num_train_epochs}, \\\n",
    "logging_steps: {logging_steps}, eval_steps: {eval_steps}, save_steps: {save_steps}\\\n",
    ", per_device_train_batch_size: {per_device_train_batch_size}, , gradient_accumulation_steps: {gradient_accumulation_steps}\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=saved_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        evaluation_strategy=\"steps\" if eval_dataset_loaded is not None else \"no\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        fp16=(compute_dtype == 'fp16'),\n",
    "        bf16=(compute_dtype == 'bf16'),\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForChatGLM(pad_token_id=tokenizer.pad_token_id)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=qlora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # ======================\n",
    "    # 6. train & auto print the train/validation loss\n",
    "    # ======================\n",
    "    trainer.train()\n",
    "\n",
    "    # ======================\n",
    "    # 7. saved the tuned model\n",
    "    # ======================\n",
    "    trainer.model.save_pretrained(saved_dir)\n",
    "    print(f\"‚úÖ fine tuned model saved to path: {saved_dir}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6409c3d9-8f9a-45cd-9390-bd1aefb41a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß Releasing GPU ...\n",
      "üîπ Loading the base model: THUDM/chatglm3-6b (not fine tuned)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d297eceab67742ce97001505a995268d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù The test text input:\n",
      "Á±ªÂûã#Ë£ô*ÁâàÂûã#ÊòæÁò¶*È£éÊ†º#ÊñáËâ∫*È£éÊ†º#ÁÆÄÁ∫¶*ÂõæÊ°à#Âç∞Ëä±*ÂõæÊ°à#ÊíûËâ≤*Ë£ô‰∏ãÊëÜ#ÂéãË§∂*Ë£ôÈïø#ËøûË°£Ë£ô*Ë£ôÈ¢ÜÂûã#ÂúÜÈ¢Ü\n",
      "\n",
      "============================================================\n",
      "üîç Evaluating base model: THUDM/chatglm3-6b (not fine tuned)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "ü§ñ Evaluating Model:Base model: THUDM/chatglm3-6b (not fine tuned)\n",
      "==================================================\n",
      "üöÄ  Start inferencing.... start_time: 2025-08-08_123442\n",
      "\n",
      "‚è±Ô∏è  Processing time (inference): 21.851 seconds\n",
      "üì§  Response:\n",
      "Ê†πÊçÆÊÇ®ÁöÑÊèèËø∞ÔºåÊàë‰∏∫ÊÇ®Êé®Ëçê‰∏ÄÊ¨æÊñáËâ∫ÁÆÄÁ∫¶ÁöÑÂç∞Ëä±ÂõæÊ°àËøûË°£Ë£ôÔºåÂÖ∑ÊúâÊòæÁò¶ÁöÑÁâàÂûãÂíå‰ºòÈõÖÁöÑÂúÜÈ¢ÜËÆæËÆ°„ÄÇË£ô‰∏ãÊëÜÈááÁî®ÂéãË§∂ËÆæËÆ°ÔºåÂ¢ûÂä†Â±ÇÊ¨°ÊÑüÂíåÊó∂Â∞öÊÑü„ÄÇÊï¥Ê¨æËøûË°£Ë£ô‰ª•ÊíûËâ≤ÂõæÊ°à‰∏∫‰∏ªÈ¢òÔºåÂ±ïÁé∞Âá∫Âà´Ê†∑ÁöÑ‰∏™ÊÄßÈ≠ÖÂäõ„ÄÇËøôÊ¨æËøûË°£Ë£ôÈÄÇÂêàÂêÑÁßçÂú∫ÂêàÔºåÂ¶ÇÁ∫¶‰ºö„ÄÅËÅö‰ºöÊàñ‰∏äÁè≠Á≠âÔºåËÆ©ÊÇ®Âú®‰ªª‰ΩïÂú∫ÂêàÈÉΩËÉΩÂ±ïÁé∞Âá∫‰ºòÈõÖÁöÑÊ∞îË¥®„ÄÇ\n",
      "\n",
      "Ë¥≠‰π∞ËøôÊ¨æËøûË°£Ë£ôÔºåÊÇ®ÂèØ‰ª•ËÄÉËôë‰ª•‰∏ãÂïÜÂÆ∂ÂíåÂπ≥Âè∞Ôºö\n",
      "1. Â§ßÂûãÁîµÂïÜÂπ≥Âè∞ÔºöÂ¶ÇÊ∑òÂÆù„ÄÅ‰∫¨‰∏úÁ≠âÔºåÊÇ®ÂèØ‰ª•ÊØîËæÉÂ§ö‰∏™ÂïÜÂÆ∂ÁöÑ‰ª∑Ê†ºÂíåÊúçÂä°ÔºåÈÄâÊã©‰∏ÄÂÆ∂‰ø°Ë™âËâØÂ•ΩÁöÑÂïÜÂÆ∂Ë¥≠‰π∞„ÄÇ\n",
      "2. ÂÆòÊñπÂìÅÁâåÂïÜÂüéÔºöÂ¶ÇÂìÅÁâåÂÆòÊñπÂïÜÂüé„ÄÅÂÆòÊñπÁΩëÁ´ôÁ≠âÔºåÁ°Æ‰øùË¥≠‰π∞Âà∞Ê≠£ÂìÅÔºå‰∫´ÂèóÂîÆÂêé‰øùÈöú„ÄÇ\n",
      "3. Êó∂Â∞öÂçö‰∏ªÊàñÁΩëÁ∫¢Â∫óÈì∫ÔºöÂèÇËÄÉÊó∂Â∞öÂçö‰∏ªÁöÑÊê≠ÈÖçÊñáÁ´†ÂíåÁΩëÁ∫¢Â∫óÈì∫ÁöÑÊé®ËçêÔºåËé∑ÂèñÊõ¥Â§öÊê≠ÈÖçÁÅµÊÑü„ÄÇ\n",
      "\n",
      "Âú®Ë¥≠‰π∞Êó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n",
      "1. ÂØπÊØîÂ§ö‰∏™ÂïÜÂÆ∂ÁöÑ‰ª∑Ê†ºÔºåÈÄâÊã©ÊÄß‰ª∑ÊØîÈ´òÁöÑ‰∫ßÂìÅ„ÄÇ\n",
      "2. ‰ªîÁªÜÈòÖËØªÂïÜÂìÅÊèèËø∞ÂíåÂõæÁâáÔºåÁ°ÆËÆ§ÂïÜÂìÅÁöÑÂìÅË¥®„ÄÅÂõæÊ°àÂíåÁâàÂûãÊòØÂê¶Á¨¶ÂêàÊÇ®ÁöÑË¶ÅÊ±Ç„ÄÇ\n",
      "3. ÂèÇËÄÉÂïÜÂÆ∂ËØÑ‰ª∑ÂíåÂÆ¢Êà∑ËØÑ‰ª∑Ôºå‰∫ÜËß£ÂÖ∂‰ªñË¥≠‰π∞ËÄÖÂØπËØ•ÂïÜÂìÅÁöÑËØÑ‰ª∑ÂíåÂª∫ËÆÆ„ÄÇ\n",
      "4. Ê≥®ÊÑèÂïÜÂìÅÁöÑÂ∞∫ÂØ∏„ÄÅÈ¢úËâ≤ÂíåÊ¨æÂºèÔºåÁ°Æ‰øùÁ¨¶ÂêàÊÇ®ÁöÑÈúÄÊ±Ç„ÄÇ\n",
      "\n",
      "Â∏åÊúõÊÇ®Ë¥≠‰π∞Âà∞Êª°ÊÑèÁöÑËøûË°£Ë£ôÔºå‰∫´ÂèóÊó∂Â∞öÁöÑÁ©øÁùÄ‰ΩìÈ™åÔºÅ\n",
      "\n",
      "============================================================\n",
      "üîç Loading PEFT model from: models/finetuned_HasturOfficial_adgen\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "ü§ñ Evaluating Model:Fine tuned model (models/finetuned_HasturOfficial_adgen)\n",
      "==================================================\n",
      "üöÄ  Start inferencing.... start_time: 2025-08-08_123504\n",
      "\n",
      "‚è±Ô∏è  Processing time (inference): 5.921 seconds\n",
      "üì§  Response:\n",
      "ÁÆÄÁ∫¶ÁöÑÂúÜÈ¢ÜËÆæËÆ°ÔºåÂá∏ÊòæÂá∫Â•≥ÊÄßÁöÑÊüîÁæéÔºåÊíûËâ≤ÁöÑÂ∞èÁ¢éËä±Âç∞Ëä±Ôºå‰∏∫Êï¥‰ΩìÈÄ†ÂûãÂ¢ûÊ∑ª‰∏ÄÊäπÂ∞èÊ∏ÖÊñ∞ÁöÑÊ∞îÊÅØÔºåÂèàÂ∏¶Êúâ‰∏Ä‰∏ùÊñáËâ∫Ê∞îÊÅØ„ÄÇÁü≠Ê¨æÁöÑËÆæËÆ°ÔºåÁ™ÅÊòæÂá∫Â•≥ÊÄßÁöÑÈ´òÊåëË∫´ÊùêÔºåÂèàÊòæÂæóÂçÅÂàÜÊ∏ÖÁàΩÔºåËÖ∞ÈÉ®ÁöÑÂéãË§∂ËÆæËÆ°ÔºåÂá∏ÊòæÂá∫Â•≥ÊÄßË∫´ÊùêÁöÑÊõ≤Á∫øÁæéÊÑüÔºåÂèàÂçÅÂàÜÊòæÁò¶„ÄÇ\n",
      "\n",
      "============================================================\n",
      "üîç Loading PEFT model from: models/finetuned_shibing624_AdvertiseGen\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "ü§ñ Evaluating Model:Fine tuned model (models/finetuned_shibing624_AdvertiseGen)\n",
      "==================================================\n",
      "üöÄ  Start inferencing.... start_time: 2025-08-08_123510\n",
      "\n",
      "‚è±Ô∏è  Processing time (inference): 5.345 seconds\n",
      "üì§  Response:\n",
      "ÂúÜÈ¢ÜÁöÑËøûË°£Ë£ôÂæàÈÄÇÂêàÂ§èÂ≠£Á©øÁùÄÔºåÂÆÉËÉΩÂ§üÈú≤Âá∫È¢àÈÉ®Á∫øÊù°ÔºåÂ¢ûÊ∑ªÊ∞îË¥®„ÄÇËøôÊ¨æËøûË°£Ë£ôÈááÁî®ÁÆÄÁ∫¶ÁöÑÂúÜÈ¢ÜËÆæËÆ°ÔºåÊê≠ÈÖçÊíûËâ≤<UNK>ÁöÑÂç∞Ëä±ÂõæÊ°àÔºåÊòæÂæóÊñáËâ∫ÂèàÂà´Ëá¥„ÄÇÂêåÊó∂ÔºåË£ôË∫´ÈááÁî®Ë§∂Áö±ÁöÑÂéãË§∂ËÆæËÆ°ÔºåËÆ©Ë£ôË∫´Êõ¥Âä†Á´ã‰ΩìÊúâÂûã„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "# this cell is to evaluate the modles\n",
    "def freeup_mem():\n",
    "    print(\"\\nüîß Releasing GPU ...\")\n",
    "    import torch, gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    %reset -f\n",
    "# call to freeup mem    \n",
    "freeup_mem()\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import time\n",
    "\n",
    "# ======================\n",
    "# 1. parameters setup\n",
    "# ======================\n",
    "\n",
    "# the base model\n",
    "model_name_or_path = 'THUDM/chatglm3-6b'\n",
    "\n",
    "# support multiple fine tuned models\n",
    "peft_model_paths = [\n",
    "    \"models/finetuned_HasturOfficial_adgen\",\n",
    "    \"models/finetuned_shibing624_AdvertiseGen\",\n",
    "    #can add more models for test\n",
    "]\n",
    "\n",
    "# init q_configÔºàwith 4bitÔºâ\n",
    "q_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 2. load the base model\n",
    "# ======================\n",
    "\n",
    "print(f\"üîπ Loading the base model: {model_name_or_path} (not fine tuned)...\")\n",
    "base_model = AutoModel.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=q_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")\n",
    "base_model.requires_grad_(False)\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)\n",
    "\n",
    "# ======================\n",
    "# 3. the test text input\n",
    "# ======================\n",
    "input_text = 'Á±ªÂûã#Ë£ô*ÁâàÂûã#ÊòæÁò¶*È£éÊ†º#ÊñáËâ∫*È£éÊ†º#ÁÆÄÁ∫¶*ÂõæÊ°à#Âç∞Ëä±*ÂõæÊ°à#ÊíûËâ≤*Ë£ô‰∏ãÊëÜ#ÂéãË§∂*Ë£ôÈïø#ËøûË°£Ë£ô*Ë£ôÈ¢ÜÂûã#ÂúÜÈ¢Ü'\n",
    "print(f\"\\nüìù The test text input:\\n{input_text}\")\n",
    "\n",
    "# ======================\n",
    "# 4. function to evaluate the model\n",
    "# ======================\n",
    "def evaluate_model(model_title, model, tokenizer, input_text):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ü§ñ Evaluating Model:{model_title}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        start_time_formated = time.strftime(\"%Y-%m-%d_%H%M%S\", time.localtime(start_time))\n",
    "        print(f\"üöÄ  Start inferencing.... start_time: {start_time_formated}\")\n",
    "        model.eval()\n",
    "        response, history = model.chat(tokenizer=tokenizer, query=input_text)\n",
    "        end_time = time.time()  # Record end time\n",
    "        # Calculate and print processing time\n",
    "        processing_time_sec = end_time - start_time\n",
    "        print(f\"\\n‚è±Ô∏è  Processing time (inference): {processing_time_sec:.3f} seconds\")        \n",
    "        print(f\"üì§  Response:\\n{response}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to evaluate model {model_title} with error:{e}\")\n",
    "\n",
    "# ======================\n",
    "# 5.1. Evaluate base model\n",
    "# ======================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üîç Evaluating base model: {model_name_or_path} (not fine tuned)\")\n",
    "print(\"=\"*60)\n",
    "evaluate_model(f\"Base model: {model_name_or_path} (not fine tuned)\", base_model, tokenizer, input_text)    \n",
    "\n",
    "# ======================\n",
    "# 5.2. Evaluating fine tuned models\n",
    "# ======================\n",
    "for peft_model_dir in peft_model_paths:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"üîç Loading PEFT model from: {peft_model_dir}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Load the PEFT adapter with the base_model\n",
    "        peft_model = PeftModel.from_pretrained(base_model, peft_model_dir)\n",
    "        evaluate_model(f\"Fine tuned model ({peft_model_dir})\", peft_model, tokenizer, input_text)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load the model: {peft_model_dir} with error: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53406991-77b9-4bd6-b931-6b9b4befbb45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0115e573b6ea42e587a37ec4fa565246": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bba2a18b845347c5bbb12877bd5aeb63",
       "style": "IPY_MODEL_c4bdeb35823d400faa6dd6504a6fd9e1",
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "0e014b682b9c44829c6928bab097743a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2063793972c84ed29eebfb1c28d298c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2172212d21504385b688beb1f8c748ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2b5ec3dee3944b00bf031edb6f43d1c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "41a4a976db994148af24108bd2efa510": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_db60125394004446bc376ea745db503b",
       "style": "IPY_MODEL_2172212d21504385b688beb1f8c748ee",
       "value": "‚Äá7/7‚Äá[00:05&lt;00:00,‚Äá‚Äá1.47it/s]"
      }
     },
     "42a7fd92f06b414c942cd11e561a49a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4c12d3aaf7aa481e8c3ec60a642ee122": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "531798e144a043c083d8961c94b3c307": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "847e0f04b1184588a67787a47816bcba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90001e3406fb46db9414380a3bc9071a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9369caf17ce34fb3bb46d0e1a4dbc0f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2063793972c84ed29eebfb1c28d298c0",
       "style": "IPY_MODEL_847e0f04b1184588a67787a47816bcba",
       "value": "‚Äá7/7‚Äá[00:05&lt;00:00,‚Äá‚Äá1.38it/s]"
      }
     },
     "b336d297ae5c46588e422097de6be97d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_0115e573b6ea42e587a37ec4fa565246",
        "IPY_MODEL_b9062d7473bf4f2c8ebeaf66061e55b7",
        "IPY_MODEL_9369caf17ce34fb3bb46d0e1a4dbc0f4"
       ],
       "layout": "IPY_MODEL_42a7fd92f06b414c942cd11e561a49a6"
      }
     },
     "b9062d7473bf4f2c8ebeaf66061e55b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_2b5ec3dee3944b00bf031edb6f43d1c0",
       "max": 7,
       "style": "IPY_MODEL_0e014b682b9c44829c6928bab097743a",
       "value": 7
      }
     },
     "bba2a18b845347c5bbb12877bd5aeb63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bbb3b3dd88874b0999c3d82097691389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bd53cc913acc48d9aa7711baa195ff79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4c12d3aaf7aa481e8c3ec60a642ee122",
       "max": 7,
       "style": "IPY_MODEL_bbb3b3dd88874b0999c3d82097691389",
       "value": 7
      }
     },
     "bdf6f654d0514e8f8ec6f7cecd1c183e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_90001e3406fb46db9414380a3bc9071a",
       "style": "IPY_MODEL_cf7c0f5377c34eeb975392eddb3092a8",
       "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
      }
     },
     "c4bdeb35823d400faa6dd6504a6fd9e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cf7c0f5377c34eeb975392eddb3092a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d297eceab67742ce97001505a995268d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bdf6f654d0514e8f8ec6f7cecd1c183e",
        "IPY_MODEL_bd53cc913acc48d9aa7711baa195ff79",
        "IPY_MODEL_41a4a976db994148af24108bd2efa510"
       ],
       "layout": "IPY_MODEL_531798e144a043c083d8961c94b3c307"
      }
     },
     "db60125394004446bc376ea745db503b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
