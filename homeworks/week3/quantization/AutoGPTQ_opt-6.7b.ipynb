{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_D9kG_efts3"
   },
   "source": [
    "# Quantization with AutoGPTQ for opt-6.7b\n",
    "\n",
    "```\n",
    "第三周作业一: \n",
    "1、使用 GPTQ 量化 OPT-6.7B 模型。课程代码（ https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AutoGPTQ_opt-2.7b.ipynb ） ------> this notebook is for this task.\n",
    "2、使用 AWQ 量化 Facebook OPT-6.7B 模型。Facebook OPT 模型地址： https://huggingface.co/facebook?search_models=opt\n",
    "课程代码： https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AWQ_opt-2.7b.ipynb\n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/quantization/AWQ-opt-125m.ipynb\n",
    "\n",
    "第三周作业二： 根据硬件资源情况，在 AdvertiseGen 数据集上使用 QLoRA 微调 ChatGLM3-6B 至少 10K examples，观察 Loss 变化情况，并对比微调前后模型输出结果。\n",
    "课程代码： \n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_qlora_chatglm.ipynb\n",
    "https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_chatglm_inference.ipynb\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "1880cdb18a7d438b8f76a2cf9d71b061",
      "3bc570100f374fca872d5f82ecb37c7c",
      "d68ff07b13044cd399d4982e6cb0e069",
      "86b65ec3d9054ff881a0110b3d07ce69",
      "a2712a9a39994d90989dea9cb0b5c549",
      "e6d6441389194757b047883c82795285",
      "5134b285e9d74fe394de3b0df4378a24",
      "6752ba954639416e94281eb3153d5ce6",
      "d281d0eee6fc47bf8ffae0a65b2844ef",
      "726e065027fe4692afc636472cc25a53",
      "1b9052ea55544078b369a88abe942a17",
      "d1f4c0fb1c8e44378a0e330d7aaf9b82",
      "2e6da10e20044181a102ec0509395075",
      "6f6780be37554de486ad2aa0c1079d08",
      "71c3a1a84487461bb16140e140f831a0",
      "92d77b8cdda142edb18e7925823f135b",
      "4f6f0bae65914ab69764932a811cdff1",
      "81a48c0fbbc548408a06eee2bb15cd98",
      "a9f229101e6a49eda5a2ecc62145781d",
      "181c67b7ca484b98a978630affcb7ddf",
      "f4292ce6c4f24dcb87be0b51935df8a2",
      "822286c9eb8d4cf1ae0200636cf015e5",
      "2ef116f4253e417fb461df6b79184692",
      "0326aff6a1854b35b0ad92f6836e7792",
      "b898db732a6d4f6a9001862c6d54fa45",
      "8970a6f89004494a9d9b7fe0c26e8b3d",
      "dae852e6cb354003a1a09d49ea19be71",
      "18338d52e9f2491b9820d531c8a410a1",
      "5989e3e43e204120a4073284986b4789",
      "b24d7b14fe6b482e9d2a36c123d73c1d",
      "e1c6b64a39ae422eb350fdee2cb79d68",
      "8e418b79775141c6839ec7ce366eede6",
      "ae6019d6385745978e18d3df6899ca75",
      "f4efacc1846545afa43850b3b9a0c701",
      "3a803fa0c5a94f3c9a7481603c024cbf",
      "d980cdfae6e5407c92c78d670f527dbf",
      "ab0d119687824938b0d5b6b5207806b2",
      "b3403daded6a4d37823d8ad758acca57",
      "590144bdfd824514b8603e50c00aa1d0",
      "ad530f96f2504ff5b8f14026d1cc34f9",
      "dbe1daa0ba564c189e6479d15166559e",
      "311246c2547548aead7033adec0cbd64",
      "132668df180e4ddf86fc23f19cdf913c",
      "5194d9be91eb45a2988eb17dffeb27b9",
      "cbc2319120e74be38cd38301a59e0ea5",
      "33874bd1cce04305b75ad9044f08dc4d",
      "cddef3367b62419abd4b7ae138d0d618",
      "7ead5371f54549f5819901ea2c262bac",
      "3fe09d88876d425db9c6c1706e76dc12",
      "dfdc242b63574f2eb0cc2370e441545e",
      "e21e50fd996549e79f3e630ac8abd9fe",
      "577bb7b27be04a47ab22bceec7ebe41b",
      "c0c02663859d49a59a927dad1c02fc89",
      "00ac2a926c0c460fa643e157b254b649",
      "9853c963f7de43f1a8840b7f6405e972",
      "8b7f95359d284c349bf2577a18c015cf",
      "2a6f4e0b10c84365a9ceb8478197aa94",
      "b50ad1c48bdb444f936bf6eb9ef16f32",
      "fc8549f4ce2d44679d71702c6c6831c1",
      "fe3730f721c1441897b0a8e1f1e82764",
      "392786affd2441a3b5e8f274eb0c903a",
      "2f06776b9fe647b28f3dd67734d0fa4b",
      "77aaa5569c2344f4844fdad5ce90f8a9",
      "16edfc3a246540b8a547a1f9ce51c0d1",
      "81d3c059e19341a5a120657ffdf1fc28",
      "375445fc761e44ccb33779e9ec60fe71",
      "4f81d19cb86849d59dfaebfbe2a10eea",
      "85dbd059e38343fbb38bf36006f3c9e1",
      "d2440685af434c9d927e9d810a6b423c",
      "6229c5c5d6184bc3b2af59edb4ee5599",
      "72bfadacbf154af0a637450035c65c42",
      "ecd42cbb519b4dfc8013dd72897b62c2",
      "4a673ed256864c3c82c4392cb7b6b261",
      "a69497b9c323430fae0b179d742993e1",
      "12f851dee22d4bffae5ae2c1126ec291",
      "369230435faf4320b05862e330cd2742",
      "c986acc6b50d4f4a9d27ee24e405e0d9",
      "566f2b9b1bef400d8f40f7498b3df8af",
      "fa5eebfd4c6a44f4abc6c171b3bdb4bf",
      "609349a1c50346b7955da961d0f4a9ab",
      "ead76ed1e47b47b38d015e58791e87cf",
      "c922bbbeed854f3fbbf3e324b0725bfe",
      "28ec0d5c26e24b0ba4d6ffb7222d7040",
      "bb382755558f41b8ac6b83d47dc0c06f",
      "a8a6b5b41d85461c8aaa7e7035dea8a3",
      "b9921b8bfbae426aba2bf658d3c5a5ff",
      "7f75ae4dd3164387b6128dcad4d8619b",
      "2b4f2f4516f04bb3b6a4f3aea71fdfac",
      "c7f13e84feaf44d694f70395a1fed7b3",
      "bbefcb7d560245f0b945741bb059d9e2",
      "8fefab0d44ab459ba870b91088901cc3",
      "ea42b0c8afba43ef83952ff1275357bc",
      "7257323c093d4aecad16b1c73963d99d",
      "be07ced9b72d417c9ef860f05205819d",
      "8cfb78d32c9d495384a37f2adad4f214",
      "c8c62f2212524da5ad8d15120a5994af",
      "720906051c7d4ba9be711d4dd265a237",
      "e0f9e91b03e641358461f2d910d2ffcc",
      "b41570ee27f14e0bad1364f5176976ea",
      "ff74dc2286464c7b8884c4702bdae74a",
      "5a54f4b13eb94d01bd769c5d48c94729",
      "cf9710af4c6c44e6bede27461724e6d2",
      "8c3a3257917a4c32b310146a447a7c54",
      "7df99f335e634352a6960e0966b57696",
      "8fe1cfba38a34a3d99b23d05cdd181d3",
      "cb18da2d2b8c41f1823b1689a8962179",
      "5a5ac3f8776f4372925fe601c63be684",
      "ec5b09e8599444e6ae9b22654f479f83",
      "f0f405585d184647afe9faf9a8f12bdc",
      "9cff1286ba114ac195600f89142c38f8",
      "af41858510eb478d8d9c69f8947c5261",
      "dcbc3809225c4c9485b5b680281a727f",
      "34e740eb3dd549d988d0b8c704f221d1",
      "d6ba78ca0f6f4e97b924b67d6da365db",
      "2fe019ca9a944f5fb73da1dd82c5579a",
      "b22a80bb6ed54aa988dfabc09c164794",
      "221f1d6f10774d5397ae21ecba375e99",
      "4c833e48ebc7486aa922e4b67782df18",
      "f82cf57aa6a74e8f85c233c63510588a",
      "ab73fb1b09904e389025fdc764830e24",
      "1e9b517d49354f3c8b722413267d9ba1",
      "c9ab1ea09ddc45db9c649bb0db4d4f4b",
      "2dec347917aa43a4af74869cb0d0751b",
      "526735fa31114c58b4e03f4648c59988",
      "9f4d37d31cc24e13a88548be94cb8dd5",
      "3a9d93fccc554551a5339936ff37f4f7",
      "5dbc8d8b58a7451aa442923587113e27",
      "381f6d6fc4e14b639d848c2321df9aea",
      "07cd986bd7bf4b2e91df6361df5e8cd2",
      "1c590dd7fd2d4b87a02e56a73e8c6cd2",
      "af8fb5b3924f4a148309b262fb31bcc5",
      "0e9bf4e276674429be46e818d6291a87",
      "097f3a4779a74e3180f66560b0347538",
      "f48fa46b4275460eb8c9092a9797ab2a",
      "ef0792efd8b64317b2cbf0e6c2504306",
      "39ae5f7f27204a6b9fcda0b716998417",
      "5cd2d916a67742b28be710135bdbe3fa",
      "0e1b96fb7a08458e801c3b162789b5bb",
      "6647de8a3df94a278d108144c64d969e",
      "286cb3c79d8747b9b1e5de3ab1aff4d5",
      "95080788a5424cd597e2c5c204535ff0",
      "891a9558c5804916a033fee94d06599c",
      "b49e5c81472d4aa895799d70890d61b6",
      "0f5c4ef239d44c02a9c1c53117c12fbc",
      "c2ce4fb2bea342b6b02635d1adc63b96",
      "ca2cdea1d81f4671ad2d4d0a1147f931",
      "f6bb7d77160b481d939c9faa21f61f49",
      "497b8e88c1534c879166fc1b0ecb4184",
      "77cfbcec7a39470ab8d142cd67dcc676",
      "bd159193f35e43aca8ad6244c497ac44",
      "c2028ebe67824dee9932c196679c6920",
      "2a121a81a37943bf99d3cd92e961cedc",
      "9876d27262c6400b88416dd43f3f6c4f",
      "8f4991db20864f48aa025c0f75b4b621",
      "17a4f65808b94b7cb637a4ddddd227e5",
      "c352f759377b4a8ca3bad246dc1a2dca",
      "598e4fa641ec491ebb5f26122ada5d6b",
      "6736dd8ceb3b4aca9ab0b938614f9075",
      "f68b5eead14e4b3a811afc3cb9f57ed8",
      "80952a605cc0468b9e6fb429ed299539",
      "5cf71707e6384f36a8dc8856e6c7b39c",
      "f0be42152fe743efafef518a061aba25",
      "545ab0836c0a43b5b5745f23ff798958",
      "f4494dd20e43415a8016b0fdeb242cff",
      "7956965b0d234142a07f2c1335f19428",
      "6388485b85b241d6839a1addc0004d81",
      "3beeabc17cdd4d30b1c1384d3ff23e7b",
      "d4a027ebea7c425c86f51e1bb55926c5",
      "0aa92a076cac40f7bc17a72490455968",
      "5530b0a841fe4820845e48558fdb994a",
      "249ec68bb4a34a1299dfbf2bc00d3813",
      "0323969428f04db7bb8df5a44b2020e5",
      "dda41887c13d445289fe0068bc640545",
      "830d39cf23b14b538282da200adfc71a",
      "1bc543af85e440649fa963bcb5083b55",
      "4c5d443d664b473faea1c8098d1e8ce3",
      "31518489c13b4039a9fc03ca0fffa933",
      "65da4d3df6c44299b97a7cefc137d2ef",
      "68da191926d44c60bf7e91f253c97ab9",
      "79f62289aeaa45ad9241e5046970a56a",
      "b058e34960e941e3aebc8f61b1fd5cda",
      "ceafeec551f543039d7d1d953c2d54d4",
      "87b19fdbd10147638fc14c91f14403b9",
      "12cb229b02ed443da4305af5f1742531",
      "1f44c06e2aa44422a542c3a608b45808",
      "ba269ea147324c7c9ee419e2d99045bf",
      "5d47b0910292477fb9f10462d1fa831b",
      "235090a2c98c4a9484f4988cf7f003f5",
      "38b6d0e2c6db4feabee91af07a88ec3c",
      "a31e6da7013643c48a7225726313fa55",
      "1a2818686dd44374a54f3aaf439e37b3",
      "c314f57a0b0b451aad29e9fbde5385e1",
      "946f1b4aff26458493e270bbb4136d75",
      "cbc18ed207314bfabfa9cccc975c146f",
      "4d2c30c3d9c14ccaba8434993e049c97",
      "bce94beb47834bcea253a6b5bf12b435",
      "c59bdca7575d4c9d9b1cb4f80dc130a7",
      "a39c7540f3ca44aeac2e7484579588b0",
      "2b4c09e0dae64e76b7b9b47d2d382b25",
      "3358d452584644e6b6bb31c52f7c5371",
      "93016a890b3d43c583a1927d1e59f2a2",
      "0fbd48759430447fb4ab05fb0d7467a6",
      "13511c3ee3364f2e9d94621f20ca7328",
      "c6fb60096ed9409cacbce03d86eced11",
      "f031d1805c76486f82a9d0f8e6a1aa9f",
      "4240fd28cec94147aedeae6ac48cf50b",
      "814d245033494cd4b96829fb130bd3e0",
      "94ca2181c5a844f9b0002a12cf8c02d1",
      "2a22739f4d2148fcb70a137ec7f3702b",
      "dff3485601944cc1a4956708ba5288e9",
      "f3416ca9de19408abed068da001dba95",
      "4a7bf23d815b4e478ba8667214f7dccc",
      "a5dbede577d14b94a9d26aff86c8ce2e",
      "888a1231ca4c47f89008113d36712390",
      "2bc16b4e785644d88c7325ac1f4dcf4e",
      "612bd4ef0b75466badfa63654d017eec",
      "f54145d621cd4df8be0e80da4dac13bb",
      "35f1ed92b3ff428ea74ea3395aada797",
      "af6d993d60d44fed90601e1288f90867",
      "7ee1819aa09a4ba7b204f4b6683bcd9e",
      "0bc89951f2b24688931fc8b14610cf7e",
      "c7f066cde8824f1ea5c155e1b2b8b33b",
      "e6d3859f4e424c569e52b97289a7cb25",
      "b5261f2aacbf473688d14ea34c017899",
      "f63c36a82f7e4ef1b023f5049304c7e1",
      "d899fd79ab434f4ea0400cfdd37716f2",
      "60c2919930bb4fa0aa85af4a1296a7bb",
      "61ce448e5d344ca0a3b01e7e5c3ee15c",
      "7c3c78d176e341a4add7dd15d3bfeb2c",
      "fb8ce606d501414cbb0ab353326f13bc",
      "3d1a5e881bd34a1291edef34cd77a629",
      "da795b06bb414b6a8a45b2a546fbc68d",
      "da5e019c5a5346c9a4093d1ead8912b6",
      "cf6e275b025443b5a20dc9d627297d0b",
      "ac42fda6d5154e81a393c94ad658a46a",
      "4eda0df91473445e983deedd58ab2983",
      "1118675d45c444368cae2473d866057b",
      "991b0a14377246d6b299241bf0f6ead3",
      "8db8b51a3e5d43ee9b78ce809b451a09",
      "911af1b0fe304b7482d945c025ecdf74",
      "eced428220be4f0e82f13d0e76d618cd",
      "6d3ff118b32840a891d5b7e1cbd9fce7",
      "555aed75863b4637aa8174e4ec8a76b2",
      "fd5b5608909d40e8b9eb8241c01b223b",
      "ce7914f77e8a4398838a5667f0df0582",
      "1587ceae6a5a4d8b9fa9ea1f1081d113",
      "826bf7e8f20d4876a53a26de5018acd0",
      "f0fbd37b109d4778837aba5ddf27ba1a",
      "156a2ae5a8664526835d6e23274bf46a",
      "2a86b9d9bfbf4cc8ac46cce25c3b01c2",
      "27beeda60e2144ba82b52533234f274d",
      "07740f4a064c4d82bb86a1ee20ea9fca",
      "879c6c9a602647aa954ecca70c795557",
      "077cba201cbf436dac4ff29df159bcf2",
      "8d0e189719ef4ed39ade7ca1b660e354",
      "b8aa181525924e37b302900688a3341b",
      "fd5fcf2072df4f6aa5571dbf00297e4d",
      "058e321ee0a74cee899b0a7ee5e85dd0",
      "7ee12dfce54040988ab9223ca81b5c8f",
      "37a044abaa6843a69ab8f183937e564e",
      "a0ef8976938446378d9d79f4336c9ec0",
      "1492496deaeb44278442b6db0a2a3d26",
      "7e47af150fb4406593a92f3428bc9f6b",
      "c58046d2e6844ecd856daa5eec19b197",
      "cd3e9031b41e49b19ef10837975b831e",
      "59d34d17db45411fb37c1203fb7c9735",
      "9c8008fda5934608aad96cb82e73a384",
      "f6fffe1f25a24fa599ede7603eabba7f",
      "77684995463340e29a20da2ab825ef43",
      "22dd1e8e03e94fb89b1415986125ad19",
      "24755e3625284e8080444420266454af",
      "85137b58cc40482daa65e95c75efe090",
      "c4ac94030e534f54927d0e1bbab354ad",
      "98060f217abc4f03bf70ec32488456d9",
      "4504cda13d4241858ac49c8e9b8611d6"
     ]
    },
    "id": "gEdeadOEoRzq",
    "outputId": "f9943b07-f19e-4f41-9063-21710fbee8af"
   },
   "outputs": [],
   "source": [
    "# Imports & Function Definition - (Run once)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def run_quantized_model(\n",
    "    model_name_or_path: str,\n",
    "    output_dir: str = None,\n",
    "    bits: int = 4,\n",
    "    group_size: int = 128,\n",
    "    dataset: str = \"wikitext2\",\n",
    "    desc_act: bool = False,\n",
    "    text: str = \"Merry Christmas! I'm glad to\",\n",
    "    max_new_tokens: int = 64,\n",
    "    device_map: str = \"auto\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a quantized GPT-style model with GPTQ, save it, and test inference.\n",
    "\n",
    "    Args:\n",
    "        model_name_or_path (str): Model identifier or path (e.g., \"facebook/opt-2.7b\")\n",
    "        output_dir (str): Where to save the quantized model (default: models/<model_name>-gptq)\n",
    "        bits (int): Quantization bits (e.g., 4)\n",
    "        group_size (int): GPTQ group size\n",
    "        dataset (str): Calibration dataset name (e.g., \"wikitext2\")\n",
    "        desc_act (bool): Whether to use desc_act in GPTQ\n",
    "        text (str): Input text for generation test\n",
    "        max_new_tokens (int): Max tokens to generate\n",
    "        device_map (str): Device mapping, e.g., \"auto\"\n",
    "    \"\"\"\n",
    "    # Define quantization config\n",
    "    quantization_config = GPTQConfig(\n",
    "        bits=bits,\n",
    "        group_size=group_size,\n",
    "        dataset=dataset,\n",
    "        desc_act=desc_act\n",
    "    )\n",
    "\n",
    "    print(f\"🔃 Loading quantized model: {model_name_or_path}, using dataset: {dataset}, with bits: {bits}, with group_size: {group_size}\")\n",
    "    # Load the quantized model with GPTQ\n",
    "    quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        quantization_config=quantization_config,\n",
    "        #max_memory = {0:\"16GiB\", \"cpu\":\"30GiB\"},\n",
    "        device_map=device_map\n",
    "    )\n",
    "\n",
    "    # Inspect quantized layer (optional, for debugging)\n",
    "    first_attn_layer = quant_model.model.decoder.layers[0].self_attn.q_proj\n",
    "    print(f\"✅ Loaded quantized attention layer: {first_attn_layer.__class__.__name__}\")\n",
    "\n",
    "    # Check for qweight and qzeros properly (even if not in __dict__)\n",
    "    desired_attrs = ['qweight', 'qzeros', 'scales']  # common quantized linear layer attributes\n",
    "    \n",
    "    print(\"\\n🔎 Checking for quantized attributes (even if not in __dict__):\")\n",
    "    for attr in desired_attrs:\n",
    "        if hasattr(first_attn_layer, attr):\n",
    "            value = getattr(first_attn_layer, attr)\n",
    "            attr_type = type(value).__name__\n",
    "            is_tensor = isinstance(value, torch.Tensor)\n",
    "            dtype = value.dtype if is_tensor else \"N/A (not a tensor)\"\n",
    "            print(f\"   ✅ Found: '{attr}' | Type: {attr_type} | Tensor: {is_tensor} | Dtype: {dtype}\")\n",
    "        else:\n",
    "            print(f\"   ❌ Not found: '{attr}'\")\n",
    "\n",
    "    # Define output directory (default: models/<model>-gptq)\n",
    "    if output_dir is None:\n",
    "        model_name = model_name_or_path.split(\"/\")[-1]  # e.g., \"opt-125m\"\n",
    "        output_dir = f\"models/{model_name}-gptq\"\n",
    "\n",
    "    # Save the quantized model\n",
    "    print(f\"💾 Saving quantized model to: {output_dir}\")\n",
    "    quant_model.save_pretrained(output_dir)\n",
    "\n",
    "    # Load tokenizer (always from original model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "    # Generate text (inference test)\n",
    "    print(f\"Generating text for input: '{text}'\")\n",
    "    # --- Start timing ---\n",
    "    start_time = time.time()  # Record start time\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)  # Move to GPU 0    \n",
    "    out = quant_model.generate(**inputs, max_new_tokens=max_new_tokens)  # Generation happens here\n",
    "    \n",
    "    end_time = time.time()  # Record end time\n",
    "    # --- End timing ---\n",
    "    \n",
    "    # Calculate and print processing time\n",
    "    processing_time_sec = end_time - start_time\n",
    "    print(f\"\\n⏱️  Processing time (inference): {processing_time_sec:.3f} seconds\")\n",
    "\n",
    "    generated_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\n📝 Generated Text:\")\n",
    "    print(generated_text)\n",
    "\n",
    "    return quant_model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468,
     "referenced_widgets": [
      "1880cdb18a7d438b8f76a2cf9d71b061",
      "3bc570100f374fca872d5f82ecb37c7c",
      "d68ff07b13044cd399d4982e6cb0e069",
      "86b65ec3d9054ff881a0110b3d07ce69",
      "a2712a9a39994d90989dea9cb0b5c549",
      "e6d6441389194757b047883c82795285",
      "5134b285e9d74fe394de3b0df4378a24",
      "6752ba954639416e94281eb3153d5ce6",
      "d281d0eee6fc47bf8ffae0a65b2844ef",
      "726e065027fe4692afc636472cc25a53",
      "1b9052ea55544078b369a88abe942a17",
      "d1f4c0fb1c8e44378a0e330d7aaf9b82",
      "2e6da10e20044181a102ec0509395075",
      "6f6780be37554de486ad2aa0c1079d08",
      "71c3a1a84487461bb16140e140f831a0",
      "92d77b8cdda142edb18e7925823f135b",
      "4f6f0bae65914ab69764932a811cdff1",
      "81a48c0fbbc548408a06eee2bb15cd98",
      "a9f229101e6a49eda5a2ecc62145781d",
      "181c67b7ca484b98a978630affcb7ddf",
      "f4292ce6c4f24dcb87be0b51935df8a2",
      "822286c9eb8d4cf1ae0200636cf015e5",
      "2ef116f4253e417fb461df6b79184692",
      "0326aff6a1854b35b0ad92f6836e7792",
      "b898db732a6d4f6a9001862c6d54fa45",
      "8970a6f89004494a9d9b7fe0c26e8b3d",
      "dae852e6cb354003a1a09d49ea19be71",
      "18338d52e9f2491b9820d531c8a410a1",
      "5989e3e43e204120a4073284986b4789",
      "b24d7b14fe6b482e9d2a36c123d73c1d",
      "e1c6b64a39ae422eb350fdee2cb79d68",
      "8e418b79775141c6839ec7ce366eede6",
      "ae6019d6385745978e18d3df6899ca75",
      "f4efacc1846545afa43850b3b9a0c701",
      "3a803fa0c5a94f3c9a7481603c024cbf",
      "d980cdfae6e5407c92c78d670f527dbf",
      "ab0d119687824938b0d5b6b5207806b2",
      "b3403daded6a4d37823d8ad758acca57",
      "590144bdfd824514b8603e50c00aa1d0",
      "ad530f96f2504ff5b8f14026d1cc34f9",
      "dbe1daa0ba564c189e6479d15166559e",
      "311246c2547548aead7033adec0cbd64",
      "132668df180e4ddf86fc23f19cdf913c",
      "5194d9be91eb45a2988eb17dffeb27b9",
      "cbc2319120e74be38cd38301a59e0ea5",
      "33874bd1cce04305b75ad9044f08dc4d",
      "cddef3367b62419abd4b7ae138d0d618",
      "7ead5371f54549f5819901ea2c262bac",
      "3fe09d88876d425db9c6c1706e76dc12",
      "dfdc242b63574f2eb0cc2370e441545e",
      "e21e50fd996549e79f3e630ac8abd9fe",
      "577bb7b27be04a47ab22bceec7ebe41b",
      "c0c02663859d49a59a927dad1c02fc89",
      "00ac2a926c0c460fa643e157b254b649",
      "9853c963f7de43f1a8840b7f6405e972",
      "8b7f95359d284c349bf2577a18c015cf",
      "2a6f4e0b10c84365a9ceb8478197aa94",
      "b50ad1c48bdb444f936bf6eb9ef16f32",
      "fc8549f4ce2d44679d71702c6c6831c1",
      "fe3730f721c1441897b0a8e1f1e82764",
      "392786affd2441a3b5e8f274eb0c903a",
      "2f06776b9fe647b28f3dd67734d0fa4b",
      "77aaa5569c2344f4844fdad5ce90f8a9",
      "16edfc3a246540b8a547a1f9ce51c0d1",
      "81d3c059e19341a5a120657ffdf1fc28",
      "375445fc761e44ccb33779e9ec60fe71",
      "4f81d19cb86849d59dfaebfbe2a10eea",
      "85dbd059e38343fbb38bf36006f3c9e1",
      "d2440685af434c9d927e9d810a6b423c",
      "6229c5c5d6184bc3b2af59edb4ee5599",
      "72bfadacbf154af0a637450035c65c42",
      "ecd42cbb519b4dfc8013dd72897b62c2",
      "4a673ed256864c3c82c4392cb7b6b261",
      "a69497b9c323430fae0b179d742993e1",
      "12f851dee22d4bffae5ae2c1126ec291",
      "369230435faf4320b05862e330cd2742",
      "c986acc6b50d4f4a9d27ee24e405e0d9",
      "566f2b9b1bef400d8f40f7498b3df8af",
      "fa5eebfd4c6a44f4abc6c171b3bdb4bf",
      "609349a1c50346b7955da961d0f4a9ab",
      "ead76ed1e47b47b38d015e58791e87cf",
      "c922bbbeed854f3fbbf3e324b0725bfe",
      "28ec0d5c26e24b0ba4d6ffb7222d7040",
      "bb382755558f41b8ac6b83d47dc0c06f",
      "a8a6b5b41d85461c8aaa7e7035dea8a3",
      "b9921b8bfbae426aba2bf658d3c5a5ff",
      "7f75ae4dd3164387b6128dcad4d8619b",
      "2b4f2f4516f04bb3b6a4f3aea71fdfac",
      "c7f13e84feaf44d694f70395a1fed7b3",
      "bbefcb7d560245f0b945741bb059d9e2",
      "8fefab0d44ab459ba870b91088901cc3",
      "ea42b0c8afba43ef83952ff1275357bc",
      "7257323c093d4aecad16b1c73963d99d",
      "be07ced9b72d417c9ef860f05205819d",
      "8cfb78d32c9d495384a37f2adad4f214",
      "c8c62f2212524da5ad8d15120a5994af",
      "720906051c7d4ba9be711d4dd265a237",
      "e0f9e91b03e641358461f2d910d2ffcc",
      "b41570ee27f14e0bad1364f5176976ea",
      "ff74dc2286464c7b8884c4702bdae74a",
      "5a54f4b13eb94d01bd769c5d48c94729",
      "cf9710af4c6c44e6bede27461724e6d2",
      "8c3a3257917a4c32b310146a447a7c54",
      "7df99f335e634352a6960e0966b57696",
      "8fe1cfba38a34a3d99b23d05cdd181d3",
      "cb18da2d2b8c41f1823b1689a8962179",
      "5a5ac3f8776f4372925fe601c63be684",
      "ec5b09e8599444e6ae9b22654f479f83",
      "f0f405585d184647afe9faf9a8f12bdc",
      "9cff1286ba114ac195600f89142c38f8",
      "af41858510eb478d8d9c69f8947c5261",
      "dcbc3809225c4c9485b5b680281a727f",
      "34e740eb3dd549d988d0b8c704f221d1",
      "d6ba78ca0f6f4e97b924b67d6da365db",
      "2fe019ca9a944f5fb73da1dd82c5579a",
      "b22a80bb6ed54aa988dfabc09c164794",
      "221f1d6f10774d5397ae21ecba375e99",
      "4c833e48ebc7486aa922e4b67782df18",
      "f82cf57aa6a74e8f85c233c63510588a",
      "ab73fb1b09904e389025fdc764830e24",
      "1e9b517d49354f3c8b722413267d9ba1",
      "c9ab1ea09ddc45db9c649bb0db4d4f4b",
      "2dec347917aa43a4af74869cb0d0751b",
      "526735fa31114c58b4e03f4648c59988",
      "9f4d37d31cc24e13a88548be94cb8dd5",
      "3a9d93fccc554551a5339936ff37f4f7",
      "5dbc8d8b58a7451aa442923587113e27",
      "381f6d6fc4e14b639d848c2321df9aea",
      "07cd986bd7bf4b2e91df6361df5e8cd2",
      "1c590dd7fd2d4b87a02e56a73e8c6cd2",
      "af8fb5b3924f4a148309b262fb31bcc5",
      "0e9bf4e276674429be46e818d6291a87",
      "097f3a4779a74e3180f66560b0347538",
      "f48fa46b4275460eb8c9092a9797ab2a",
      "ef0792efd8b64317b2cbf0e6c2504306",
      "39ae5f7f27204a6b9fcda0b716998417",
      "5cd2d916a67742b28be710135bdbe3fa",
      "0e1b96fb7a08458e801c3b162789b5bb",
      "6647de8a3df94a278d108144c64d969e",
      "286cb3c79d8747b9b1e5de3ab1aff4d5",
      "95080788a5424cd597e2c5c204535ff0",
      "891a9558c5804916a033fee94d06599c",
      "b49e5c81472d4aa895799d70890d61b6",
      "0f5c4ef239d44c02a9c1c53117c12fbc",
      "c2ce4fb2bea342b6b02635d1adc63b96",
      "ca2cdea1d81f4671ad2d4d0a1147f931",
      "f6bb7d77160b481d939c9faa21f61f49",
      "497b8e88c1534c879166fc1b0ecb4184",
      "77cfbcec7a39470ab8d142cd67dcc676",
      "bd159193f35e43aca8ad6244c497ac44",
      "c2028ebe67824dee9932c196679c6920",
      "2a121a81a37943bf99d3cd92e961cedc",
      "9876d27262c6400b88416dd43f3f6c4f",
      "8f4991db20864f48aa025c0f75b4b621",
      "17a4f65808b94b7cb637a4ddddd227e5",
      "c352f759377b4a8ca3bad246dc1a2dca",
      "598e4fa641ec491ebb5f26122ada5d6b",
      "6736dd8ceb3b4aca9ab0b938614f9075",
      "f68b5eead14e4b3a811afc3cb9f57ed8",
      "80952a605cc0468b9e6fb429ed299539",
      "5cf71707e6384f36a8dc8856e6c7b39c",
      "f0be42152fe743efafef518a061aba25",
      "545ab0836c0a43b5b5745f23ff798958",
      "f4494dd20e43415a8016b0fdeb242cff",
      "7956965b0d234142a07f2c1335f19428",
      "6388485b85b241d6839a1addc0004d81",
      "3beeabc17cdd4d30b1c1384d3ff23e7b",
      "d4a027ebea7c425c86f51e1bb55926c5",
      "0aa92a076cac40f7bc17a72490455968",
      "5530b0a841fe4820845e48558fdb994a",
      "249ec68bb4a34a1299dfbf2bc00d3813",
      "0323969428f04db7bb8df5a44b2020e5",
      "dda41887c13d445289fe0068bc640545",
      "830d39cf23b14b538282da200adfc71a",
      "1bc543af85e440649fa963bcb5083b55",
      "4c5d443d664b473faea1c8098d1e8ce3",
      "31518489c13b4039a9fc03ca0fffa933",
      "65da4d3df6c44299b97a7cefc137d2ef",
      "68da191926d44c60bf7e91f253c97ab9",
      "79f62289aeaa45ad9241e5046970a56a",
      "b058e34960e941e3aebc8f61b1fd5cda",
      "ceafeec551f543039d7d1d953c2d54d4",
      "87b19fdbd10147638fc14c91f14403b9",
      "12cb229b02ed443da4305af5f1742531",
      "1f44c06e2aa44422a542c3a608b45808",
      "ba269ea147324c7c9ee419e2d99045bf",
      "5d47b0910292477fb9f10462d1fa831b",
      "235090a2c98c4a9484f4988cf7f003f5",
      "38b6d0e2c6db4feabee91af07a88ec3c",
      "a31e6da7013643c48a7225726313fa55",
      "1a2818686dd44374a54f3aaf439e37b3",
      "c314f57a0b0b451aad29e9fbde5385e1",
      "946f1b4aff26458493e270bbb4136d75",
      "cbc18ed207314bfabfa9cccc975c146f",
      "4d2c30c3d9c14ccaba8434993e049c97",
      "bce94beb47834bcea253a6b5bf12b435",
      "c59bdca7575d4c9d9b1cb4f80dc130a7",
      "a39c7540f3ca44aeac2e7484579588b0",
      "2b4c09e0dae64e76b7b9b47d2d382b25",
      "3358d452584644e6b6bb31c52f7c5371",
      "93016a890b3d43c583a1927d1e59f2a2",
      "0fbd48759430447fb4ab05fb0d7467a6",
      "13511c3ee3364f2e9d94621f20ca7328",
      "c6fb60096ed9409cacbce03d86eced11",
      "f031d1805c76486f82a9d0f8e6a1aa9f",
      "4240fd28cec94147aedeae6ac48cf50b",
      "814d245033494cd4b96829fb130bd3e0",
      "94ca2181c5a844f9b0002a12cf8c02d1",
      "2a22739f4d2148fcb70a137ec7f3702b",
      "dff3485601944cc1a4956708ba5288e9",
      "f3416ca9de19408abed068da001dba95",
      "4a7bf23d815b4e478ba8667214f7dccc",
      "a5dbede577d14b94a9d26aff86c8ce2e",
      "888a1231ca4c47f89008113d36712390",
      "2bc16b4e785644d88c7325ac1f4dcf4e",
      "612bd4ef0b75466badfa63654d017eec",
      "f54145d621cd4df8be0e80da4dac13bb",
      "35f1ed92b3ff428ea74ea3395aada797",
      "af6d993d60d44fed90601e1288f90867",
      "7ee1819aa09a4ba7b204f4b6683bcd9e",
      "0bc89951f2b24688931fc8b14610cf7e",
      "c7f066cde8824f1ea5c155e1b2b8b33b",
      "e6d3859f4e424c569e52b97289a7cb25",
      "b5261f2aacbf473688d14ea34c017899",
      "f63c36a82f7e4ef1b023f5049304c7e1",
      "d899fd79ab434f4ea0400cfdd37716f2",
      "60c2919930bb4fa0aa85af4a1296a7bb",
      "61ce448e5d344ca0a3b01e7e5c3ee15c",
      "7c3c78d176e341a4add7dd15d3bfeb2c",
      "fb8ce606d501414cbb0ab353326f13bc",
      "3d1a5e881bd34a1291edef34cd77a629",
      "da795b06bb414b6a8a45b2a546fbc68d",
      "da5e019c5a5346c9a4093d1ead8912b6",
      "cf6e275b025443b5a20dc9d627297d0b",
      "ac42fda6d5154e81a393c94ad658a46a",
      "4eda0df91473445e983deedd58ab2983",
      "1118675d45c444368cae2473d866057b",
      "991b0a14377246d6b299241bf0f6ead3",
      "8db8b51a3e5d43ee9b78ce809b451a09",
      "911af1b0fe304b7482d945c025ecdf74",
      "eced428220be4f0e82f13d0e76d618cd",
      "6d3ff118b32840a891d5b7e1cbd9fce7",
      "555aed75863b4637aa8174e4ec8a76b2",
      "fd5b5608909d40e8b9eb8241c01b223b",
      "ce7914f77e8a4398838a5667f0df0582",
      "1587ceae6a5a4d8b9fa9ea1f1081d113",
      "826bf7e8f20d4876a53a26de5018acd0",
      "f0fbd37b109d4778837aba5ddf27ba1a",
      "156a2ae5a8664526835d6e23274bf46a",
      "2a86b9d9bfbf4cc8ac46cce25c3b01c2",
      "27beeda60e2144ba82b52533234f274d",
      "07740f4a064c4d82bb86a1ee20ea9fca",
      "879c6c9a602647aa954ecca70c795557",
      "077cba201cbf436dac4ff29df159bcf2",
      "8d0e189719ef4ed39ade7ca1b660e354",
      "b8aa181525924e37b302900688a3341b",
      "fd5fcf2072df4f6aa5571dbf00297e4d",
      "058e321ee0a74cee899b0a7ee5e85dd0",
      "7ee12dfce54040988ab9223ca81b5c8f",
      "37a044abaa6843a69ab8f183937e564e",
      "a0ef8976938446378d9d79f4336c9ec0",
      "1492496deaeb44278442b6db0a2a3d26",
      "7e47af150fb4406593a92f3428bc9f6b",
      "c58046d2e6844ecd856daa5eec19b197",
      "cd3e9031b41e49b19ef10837975b831e",
      "59d34d17db45411fb37c1203fb7c9735",
      "9c8008fda5934608aad96cb82e73a384",
      "f6fffe1f25a24fa599ede7603eabba7f",
      "77684995463340e29a20da2ab825ef43",
      "22dd1e8e03e94fb89b1415986125ad19",
      "24755e3625284e8080444420266454af",
      "85137b58cc40482daa65e95c75efe090",
      "c4ac94030e534f54927d0e1bbab354ad",
      "98060f217abc4f03bf70ec32488456d9",
      "4504cda13d4241858ac49c8e9b8611d6"
     ]
    },
    "id": "gEdeadOEoRzq",
    "outputId": "f9943b07-f19e-4f41-9063-21710fbee8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔃 Loading quantized model: facebook/opt-6.7b, using dataset: ['ok'], with bits: 8, with group_size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9d7bd9325740ad829c2418e1ad50fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3a203857e3432a8dd73f8aa0a62fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff19d747ffd417db06fe2a0b262fa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     12\u001b[39m custom_dataset = [\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m#\"GPTQ is a powerful 4-bit quantization algorithm for large language models.\",\u001b[39;00m\n\u001b[32m     15\u001b[39m ]\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# run with wikitext2 dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m quant_model, tokenizer = \u001b[43mrun_quantized_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME_OR_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional:\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# output_dir=\"models/my-quantized-model\",\u001b[39;49;00m\n\u001b[32m     22\u001b[39m \u001b[43m     \u001b[49m\u001b[43mbits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m     \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m     \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# desc_act=False,\u001b[39;49;00m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# text=\"Hello, how are you?\",\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# max_new_tokens=32,\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# run with custom dataset\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#run_quantized_model(\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m#model_name_or_path=MODEL_NAME_OR_PATH,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# max_new_tokens=100,\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m#)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mrun_quantized_model\u001b[39m\u001b[34m(model_name_or_path, output_dir, bits, group_size, dataset, desc_act, text, max_new_tokens, device_map)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔃 Loading quantized model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, using dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, with bits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, with group_size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Load the quantized model with GPTQ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m quant_model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#max_memory = {0:\"16GiB\", \"cpu\":\"30GiB\"},\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Inspect quantized layer (optional, for debugging)\u001b[39;00m\n\u001b[32m     49\u001b[39m first_attn_layer = quant_model.model.decoder.layers[\u001b[32m0\u001b[39m].self_attn.q_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    565\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    570\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    571\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    572\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/transformers/modeling_utils.py:3924\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.main_input_name != \u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3923\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mWe can only quantize pure text model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3924\u001b[39m \u001b[43mquantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3925\u001b[39m config.quantization_config = GPTQConfig.from_dict_optimum(quantizer.to_dict())\n\u001b[32m   3926\u001b[39m model._is_quantized_training_enabled = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/optimum/gptq/quantizer.py:510\u001b[39m, in \u001b[36mGPTQQuantizer.quantize_model\u001b[39m\u001b[34m(self, model, tokenizer)\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset_name_list:\n\u001b[32m    509\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuantizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in block \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(blocks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m510\u001b[39m     scale, zero, g_idx = \u001b[43mgptq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfasterquant\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpercdamp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdamp_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactorder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc_act\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m     quantizers[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.block_name_to_quantize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m    514\u001b[39m         gptq[name].quantizer,\n\u001b[32m    515\u001b[39m         scale,\n\u001b[32m    516\u001b[39m         zero,\n\u001b[32m    517\u001b[39m         g_idx,\n\u001b[32m    518\u001b[39m     )\n\u001b[32m    519\u001b[39m     gptq[name].free()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/byenv/lib/python3.11/site-packages/auto_gptq/quantization/gptq.py:110\u001b[39m, in \u001b[36mGPTQ.fasterquant\u001b[39m\u001b[34m(self, blocksize, percdamp, group_size, actorder, static_groups)\u001b[39m\n\u001b[32m    108\u001b[39m diag = torch.arange(\u001b[38;5;28mself\u001b[39m.columns, device=\u001b[38;5;28mself\u001b[39m.dev)\n\u001b[32m    109\u001b[39m H[diag, diag] += damp\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m H = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m H = torch.cholesky_inverse(H)\n\u001b[32m    112\u001b[39m H = torch.linalg.cholesky(H, upper=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Run Quantization (Do Once) for the defined model\n",
    "# Change this to any GPTQ-compatible model\n",
    "# smallest model for faster test\n",
    "# MODEL_NAME_OR_PATH = \"facebook/opt-125m\"\n",
    "# MODEL_NAME_OR_PATH = \"facebook/opt-2.7b\"\n",
    "# keep OOM for this\n",
    "MODEL_NAME_OR_PATH = \"facebook/opt-6.7b\"\n",
    "\n",
    "# use custom dataset instead of \"wikitext2\" to reduce VRAM -- not works\n",
    "# try to use 8 instead as 4 is getting OutOfMemoryError -- not works\n",
    "# try to use group_size as 512 instead of 128\n",
    "custom_dataset = [\n",
    "    \"ok\",\n",
    "    #\"GPTQ is a powerful 4-bit quantization algorithm for large language models.\",\n",
    "]\n",
    "\n",
    "# run with wikitext2 dataset\n",
    "quant_model, tokenizer = run_quantized_model(\n",
    "    model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "    # Optional:\n",
    "    # output_dir=\"models/my-quantized-model\",\n",
    "     bits=8,\n",
    "     group_size=256,\n",
    "     dataset=custom_dataset,\n",
    "    # desc_act=False,\n",
    "    # text=\"Hello, how are you?\",\n",
    "    # max_new_tokens=32,\n",
    ")\n",
    "\n",
    "# run with custom dataset\n",
    "#run_quantized_model(\n",
    "    #model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "    # Optional:\n",
    "    # output_dir=\"models/my-quantized-model\",\n",
    "    # bits=4,\n",
    "    # group_size=128,\n",
    "    #dataset=[\n",
    "#    \"auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\",\n",
    "#    \"Quantization enables running large language models efficiently on consumer hardware.\",\n",
    "#    \"The quick brown fox jumps over the lazy dog.\"\n",
    "#    ],\n",
    "    # desc_act=False,\n",
    "    # text=\"Hello, how are you?\",\n",
    "    # max_new_tokens=100,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Quantized Model & Run Inference (Fast, No Quantization) - run as many times as expected\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Change this to any GPTQ-compatible model\n",
    "# smallest model for faster test\n",
    "# MODEL_NAME_OR_PATH = \"facebook/opt-125m\"\n",
    "# MODEL_NAME_OR_PATH = \"facebook/opt-2.7b\"\n",
    "# MODEL_NAME_OR_PATH = \"facebook/opt-6.7b\"\n",
    "\n",
    "model_name = MODEL_NAME_OR_PATH.split(\"/\")[-1]  # e.g., \"opt-2.7b\"\n",
    "SAVED_MODEL_DIR = f\"models/{model_name}-gptq\"\n",
    "\n",
    "# Load the tokenizer from the ORIGINAL model name/path\n",
    "print(f\"🚀 Loading tokenizer from: {MODEL_NAME_OR_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "\n",
    "# Load quantized model\n",
    "print(f\"🚀 Loading quantized model from: {SAVED_MODEL_DIR}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    SAVED_MODEL_DIR,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Infrecence\n",
    "def generate_text(text):\n",
    "    print(f\"Generating text for: {text}\")\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "    out = model.generate(**inputs, max_new_tokens=100)\n",
    "    result = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Generated text: {result}\")\n",
    "    return result\n",
    "\n",
    "# testing\n",
    "result = generate_text(\"Merry Christmas! I'm glad to\")\n",
    "result = generate_text(\"The woman worked as a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "12a5ba2827da48bdbefe7937582add69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "1ef0ca8e9f324bd0983a288bcdfce9be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "207e9f0582894a63b61a1f41bfaeb9f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8a0709d3af884c69a42bb2c2e4ee056d",
       "max": 2,
       "style": "IPY_MODEL_9b194ae0a11041d89b43b604aef43bc1",
       "value": 2
      }
     },
     "28e24cafcadc4f708ccf2827d4acf66b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2a9d7bd9325740ad829c2418e1ad50fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b594ffb6726f4377b80575ba7f18355d",
        "IPY_MODEL_207e9f0582894a63b61a1f41bfaeb9f1",
        "IPY_MODEL_4be07e43b02d4116ba3be61defd34cf0"
       ],
       "layout": "IPY_MODEL_2b80de4c63ee4ba2a34a1e53a873b2f2"
      }
     },
     "2b80de4c63ee4ba2a34a1e53a873b2f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2eca5bf5129c486786d98d5cf0d78d6f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "332b89497056438e9cb13757e03dca3f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "36fc853bcc874542ae110f78d2b81a42": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3c1da0b2d5424b70b55d2f4f1ac7721e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3e9ac064d2364b5a9bf0f0114fca5403": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_70ec42945b6d4e81a6cf321c0b481c9b",
       "style": "IPY_MODEL_98dd3b6f934644cfb3486c6da9570fd1",
       "value": " 5/6 [00:10&lt;00:02,  2.02s/it]"
      }
     },
     "413f43a7c3bd47e5bc7567131248b371": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4be07e43b02d4116ba3be61defd34cf0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_413f43a7c3bd47e5bc7567131248b371",
       "style": "IPY_MODEL_1ef0ca8e9f324bd0983a288bcdfce9be",
       "value": " 2/2 [00:04&lt;00:00,  1.89s/it]"
      }
     },
     "4cdb4dfc89b94cada11cafac76d28d49": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5e3a203857e3432a8dd73f8aa0a62fc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ba5ccdd3d73240709a750502e5a783d1",
        "IPY_MODEL_d9c4af0589e643669c6539a1a941ccbf",
        "IPY_MODEL_d41c58b67d3146518731b47e72e7eab5"
       ],
       "layout": "IPY_MODEL_e8bcaa1ab54043d28e6b65e982c2cc68"
      }
     },
     "63f3783c48f7497591ea4973c7fd97d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "70ec42945b6d4e81a6cf321c0b481c9b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8a0709d3af884c69a42bb2c2e4ee056d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "98dd3b6f934644cfb3486c6da9570fd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b194ae0a11041d89b43b604aef43bc1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9ff19d747ffd417db06fe2a0b262fa22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_aaf8ec40ffe3476da5eccbc3f15bee2c",
        "IPY_MODEL_a6d2d538d84e4144baf08638d08733c7",
        "IPY_MODEL_3e9ac064d2364b5a9bf0f0114fca5403"
       ],
       "layout": "IPY_MODEL_2eca5bf5129c486786d98d5cf0d78d6f"
      }
     },
     "a1e32af09d9e4cbeab3ed434822eb1a8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6d2d538d84e4144baf08638d08733c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_a1e32af09d9e4cbeab3ed434822eb1a8",
       "max": 6,
       "style": "IPY_MODEL_63f3783c48f7497591ea4973c7fd97d1",
       "value": 5
      }
     },
     "a6efcf7bb14546bd81a0baa277b20093": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "aaf8ec40ffe3476da5eccbc3f15bee2c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_332b89497056438e9cb13757e03dca3f",
       "style": "IPY_MODEL_e6b586611bb44b40a6c9001a1d0210d5",
       "value": "Quantizing layers inside the block:  83%"
      }
     },
     "b594ffb6726f4377b80575ba7f18355d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_36fc853bcc874542ae110f78d2b81a42",
       "style": "IPY_MODEL_a6efcf7bb14546bd81a0baa277b20093",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "ba5ccdd3d73240709a750502e5a783d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3c1da0b2d5424b70b55d2f4f1ac7721e",
       "style": "IPY_MODEL_ee91c6e957df48a0a3a65be01d4a3a3c",
       "value": "Quantizing model.decoder.layers blocks :   0%"
      }
     },
     "c42696c61b7040c6a11bcf4e510991e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d41c58b67d3146518731b47e72e7eab5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c42696c61b7040c6a11bcf4e510991e8",
       "style": "IPY_MODEL_4cdb4dfc89b94cada11cafac76d28d49",
       "value": " 0/32 [00:10&lt;?, ?it/s]"
      }
     },
     "d9c4af0589e643669c6539a1a941ccbf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "danger",
       "layout": "IPY_MODEL_28e24cafcadc4f708ccf2827d4acf66b",
       "max": 32,
       "style": "IPY_MODEL_12a5ba2827da48bdbefe7937582add69"
      }
     },
     "e6b586611bb44b40a6c9001a1d0210d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e8bcaa1ab54043d28e6b65e982c2cc68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee91c6e957df48a0a3a65be01d4a3a3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
