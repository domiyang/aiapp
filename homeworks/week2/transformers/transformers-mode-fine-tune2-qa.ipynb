{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# transformers fine tune qa for model: distilbert-base-uncased with dataset squad_v2/squad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ All outputs will be saved under: ./finetune/2025-08-02_151125\n",
      "ğŸ“š Loading dataset: squad_v2\n",
      "ğŸ” Starting first fine-tuning pass...\n",
      "ğŸ“¥ Loading base model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full dataset for training and evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8236' max='8236' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8236/8236 1:53:10, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.249400</td>\n",
       "      <td>1.243164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>1.239636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… First model saved to: ./finetune/2025-08-02_151125/first/models/distilbert-base-uncased-finetuned-first\n",
      "ğŸ” Starting second fine-tuning pass...\n",
      "ğŸ“¥ Loading model & tokenizer from: ./finetune/2025-08-02_151125/first/models/distilbert-base-uncased-finetuned-first\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a95c0ead86647928d7aa8bd3ee8725d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/130319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a62b885929494d92f17cc7c1105bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full dataset for training and evaluation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4118' max='4118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4118/4118 56:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>1.422362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Second model saved to: ./finetune/2025-08-02_151125/second/models/distilbert-base-uncased-finetuned-second\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958d46739f144db29a457c3985e2abf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8c16b761544828a3ba9213f53e6306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Evaluating model: distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1cf5d05e064cf6982f56230ab633e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 11873 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 12134 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22464c073fa4ab9b82ea7eea216ea20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results={'exact': 0.7580224037732671, 'f1': 4.2107866210294205, 'total': 11873, 'HasAns_exact': 0.033738191632928474, 'HasAns_f1': 6.94916827791538, 'HasAns_total': 5928, 'NoAns_exact': 1.4802354920100924, 'NoAns_f1': 1.4802354920100924, 'NoAns_total': 5945, 'best_exact': 50.07159100480081, 'best_exact_thresh': 0.0, 'best_f1': 50.07872774030034, 'best_f1_thresh': 0.0}\n",
      "ğŸ“Š Exact Match (EM): 0.76 | F1: 4.21\n",
      "ğŸ” Evaluating model: ./finetune/2025-08-02_151125/first/models/distilbert-base-uncased-finetuned-first\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6a88bb5560447a88dff24a76d93921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 11873 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 12134 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07467ffb26094ff69156d86261689ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results={'exact': 64.22976501305483, 'f1': 67.6844417882956, 'total': 11873, 'HasAns_exact': 64.28812415654521, 'HasAns_f1': 71.20738484352755, 'HasAns_total': 5928, 'NoAns_exact': 64.17157275021026, 'NoAns_f1': 64.17157275021026, 'NoAns_total': 5945, 'best_exact': 64.22976501305483, 'best_exact_thresh': 0.0, 'best_f1': 67.68444178829574, 'best_f1_thresh': 0.0}\n",
      "ğŸ“Š Exact Match (EM): 64.23 | F1: 67.68\n",
      "ğŸ” Evaluating model: ./finetune/2025-08-02_151125/second/models/distilbert-base-uncased-finetuned-second\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f59e6a96464379a488001fe90d58ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11873 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 11873 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 12134 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7f628063474b91b93767c6418f6fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results={'exact': 64.10342794575928, 'f1': 67.65450305950155, 'total': 11873, 'HasAns_exact': 65.84008097165992, 'HasAns_f1': 72.95241478162266, 'HasAns_total': 5928, 'NoAns_exact': 62.3717409587889, 'NoAns_f1': 62.3717409587889, 'NoAns_total': 5945, 'best_exact': 64.10342794575928, 'best_exact_thresh': 0.0, 'best_f1': 67.65450305950169, 'best_f1_thresh': 0.0}\n",
      "ğŸ“Š Exact Match (EM): 64.10 | F1: 67.65\n",
      "ğŸ§ª Running Sample QA Predictions...\n",
      "--- Sample QA from Model: distilbert-base-uncased ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: In what country is Normandy located?\n",
      "C1: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A1: Carolingian-\n",
      "GTA1: ['France', 'France', 'France', 'France']\n",
      "\n",
      "Q2: When were the Normans in Normandy?\n",
      "C2: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A2: Carolingian-\n",
      "GTA2: ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries']\n",
      "\n",
      "Q3: From which countries did the Norse originate?\n",
      "C3: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A3: Carolingian-\n",
      "GTA3: ['Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway']\n",
      "\n",
      "--- Sample QA from Model: ./finetune/2025-08-02_151125/first/models/distilbert-base-uncased-finetuned-first ---\n",
      "Q1: In what country is Normandy located?\n",
      "C1: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A1: France\n",
      "GTA1: ['France', 'France', 'France', 'France']\n",
      "\n",
      "Q2: When were the Normans in Normandy?\n",
      "C2: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A2: 10th and 11th centuries\n",
      "GTA2: ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries']\n",
      "\n",
      "Q3: From which countries did the Norse originate?\n",
      "C3: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A3: Denmark, Iceland and Norway\n",
      "GTA3: ['Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway']\n",
      "\n",
      "--- Sample QA from Model: ./finetune/2025-08-02_151125/second/models/distilbert-base-uncased-finetuned-second ---\n",
      "Q1: In what country is Normandy located?\n",
      "C1: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A1: France\n",
      "GTA1: ['France', 'France', 'France', 'France']\n",
      "\n",
      "Q2: When were the Normans in Normandy?\n",
      "C2: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A2: 10th and 11th centuries\n",
      "GTA2: ['10th and 11th centuries', 'in the 10th and 11th centuries', '10th and 11th centuries', '10th and 11th centuries']\n",
      "\n",
      "Q3: From which countries did the Norse originate?\n",
      "C3: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"...\n",
      "A3: Denmark, Iceland and Norway\n",
      "GTA3: ['Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway', 'Denmark, Iceland and Norway']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertForQuestionAnswering,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    pipeline\n",
    ")\n",
    "from transformers import DataCollatorWithPadding\n",
    "from evaluate import load  # Hugging Face evaluate library\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# %pip install transformers==4.54.0 evaluate==0.4.5\n",
    "\n",
    "# Set to False for quick testing with smaller subset\n",
    "use_full_dataset = True\n",
    "# used when use_full_dataset is False\n",
    "small_dataset_train_count=100\n",
    "small_dataset_test_count=10\n",
    "batch_size = 32\n",
    "\n",
    "# Create a unique timestamped base output directory\n",
    "ts = datetime.now().strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "output_dir_base = f\"./finetune/{ts}\"\n",
    "print(f\"ğŸ“‚ All outputs will be saved under: {output_dir_base}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Config: Choose SQuAD or SQuAD 2.0\n",
    "# -------------------------------\n",
    "squad_v2 = True  # Set False to use SQuAD 1.1\n",
    "dataset_name = \"squad_v2\" if squad_v2 else \"squad\"\n",
    "print(f\"ğŸ“š Loading dataset: {dataset_name}\")\n",
    "\n",
    "datasets_ori = load_dataset(dataset_name)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define the base name for Tokenizer & Model\n",
    "# -------------------------------\n",
    "model_name_base = \"distilbert-base-uncased\"\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "encoded_datasets = None\n",
    "eval_dataset_ori = None\n",
    "\n",
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128\n",
    "\n",
    "# -------------------------------\n",
    "# 3. preprocess the train dataset\n",
    "# -------------------------------\n",
    "def prepare_train_features(examples):\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥\n",
    "    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚\n",
    "    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\t\t\t\t\t  \n",
    "\n",
    "    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Training & Saving Models (First & Second Fine-Tuning)\n",
    "# -------------------------------\n",
    "output_dirs = {\n",
    "    \"base\": f\"{output_dir_base}/base\",\n",
    "    \"first\": f\"{output_dir_base}/first\",\n",
    "    \"second\": f\"{output_dir_base}/second\"\n",
    "}\n",
    "\n",
    "for key in output_dirs:\n",
    "    os.makedirs(output_dirs[key], exist_ok=True)\n",
    "\n",
    "def fine_tune_and_save(\n",
    "    pass_name,\n",
    "    output_dir,\n",
    "    train_epochs=3,\n",
    "    lr=3e-5,\n",
    "    batch_size=batch_size,\n",
    "    model_path=None  # Optional: path to a previously saved model (e.g., first fine-tuned)\n",
    "):\n",
    "    print(f\"ğŸ” Starting {pass_name} fine-tuning pass...\")\n",
    "    global model, tokenizer, encoded_datasets, eval_dataset_ori  # Use global variables\n",
    "    # Load model & tokenizer\n",
    "    if model_path:\n",
    "        print(f\"ğŸ“¥ Loading model & tokenizer from: {model_path}\")\n",
    "        model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "    else:\n",
    "        print(f\"ğŸ“¥ Loading base model: distilbert-base-uncased\")\n",
    "        model = DistilBertForQuestionAnswering.from_pretrained(model_name_base)\n",
    "        tokenizer = DistilBertTokenizerFast.from_pretrained(model_name_base)\n",
    "\n",
    "    # prepare the train/test data for the specific tokenizer\n",
    "    encoded_datasets = datasets_ori.map(\n",
    "        prepare_train_features,\n",
    "        batched=True,\n",
    "        remove_columns=datasets_ori[\"train\"].column_names\n",
    "    )\n",
    "\n",
    "    # define the train, eval dataset\n",
    "    if use_full_dataset:\n",
    "        print(\"Using full dataset for training and evaluation.\")\n",
    "        train_dataset=encoded_datasets[\"train\"]\n",
    "        eval_dataset=encoded_datasets[\"validation\"]\n",
    "        eval_dataset_ori = datasets_ori[\"validation\"]\n",
    "    else:\n",
    "        print(f\"Using smaller subset of examples small_dataset_train_count:{small_dataset_train_count}, small_dataset_test_count:{small_dataset_test_count} for quick train/testing.\")\n",
    "        train_dataset=encoded_datasets[\"train\"].shuffle(seed=42).select(range(small_dataset_train_count))\n",
    "        eval_dataset=encoded_datasets[\"validation\"].shuffle(seed=42).select(range(small_dataset_test_count))\n",
    "        eval_dataset_ori = datasets_ori[\"validation\"].select(range(small_dataset_test_count))  # small subset for speed\n",
    "        \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=lr,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=train_epochs,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=500,\n",
    "        save_total_limit=3,\n",
    "        fp16=True\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save model & tokenizer\n",
    "    saved_model_dir = f\"{output_dir}/models/{model_name_base}-finetuned-{pass_name}\"\n",
    "    trainer.save_model(saved_model_dir)\n",
    "    tokenizer.save_pretrained(saved_model_dir)\n",
    "    print(f\"âœ… {pass_name.capitalize()} model saved to: {saved_model_dir}\")\n",
    "\n",
    "    return saved_model_dir\n",
    "\n",
    "# Fine-tune first model from base\n",
    "first_model_dir = fine_tune_and_save(\n",
    "    pass_name=\"first\",\n",
    "    output_dir=output_dirs[\"first\"],\n",
    "    train_epochs=2,\n",
    "    lr=3e-5,\n",
    "    model_path=None  # Starts from base\n",
    ")\n",
    "# Fine-tune second model using the first fine-tuned model\n",
    "second_model_dir = fine_tune_and_save(\n",
    "    pass_name=\"second\",\n",
    "    output_dir=output_dirs[\"second\"],\n",
    "    train_epochs=1,\n",
    "    lr=2e-5,\n",
    "    model_path=first_model_dir  # Loads first fine-tuned model\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 5. EVALUATION: Exact Match (EM) & F1 using Hugging Face `evaluate`\n",
    "# -------------------------------\n",
    "\n",
    "squad_metric = load(\"squad_v2\" if squad_v2 else \"squad\")  # âš ï¸ important: use same as dataset!\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            sorted_valid_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)\n",
    "            #print(f\"sorted valid_ansers count={len(sorted_valid_answer)}, first answer={sorted_valid_answer[0] if len(sorted_valid_answer) > 0 else 'None'}\")\n",
    "            best_answer = sorted_valid_answer[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model_squad_format(model_dir, output_dir):\n",
    "    print(f\"ğŸ” Evaluating model: {model_dir}\")\n",
    "    global model, tokenizer  # Use global model and tokenizer\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)\n",
    "    \n",
    "    validation_features = eval_dataset_ori.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset_ori.column_names\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{output_dir}/results\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        do_train=False,          # we're not training\n",
    "        do_eval=False,           # not evaluating either\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    raw_predictions = trainer.predict(validation_features)\n",
    "    validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))\n",
    "\n",
    "    final_predictions = postprocess_qa_predictions(eval_dataset_ori, validation_features, raw_predictions.predictions)\n",
    "\n",
    "    if squad_v2:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "    \n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in eval_dataset_ori]\n",
    "\n",
    "    results = squad_metric.compute(predictions=formatted_predictions, references=references)\n",
    "    print(f\"results={results}\")\n",
    "    print(f\"ğŸ“Š Exact Match (EM): {results['exact']:.2f} | F1: {results['f1']:.2f}\")\n",
    "\n",
    "\n",
    "# Evaluate all 3 models\n",
    "evaluate_model_squad_format(model_name_base, output_dirs[\"base\"])  # Base model\n",
    "evaluate_model_squad_format(first_model_dir, output_dirs[\"first\"])   # First fine-tuned\n",
    "evaluate_model_squad_format(second_model_dir, output_dirs[\"second\"])  # Second fine-tuned\n",
    "\n",
    "# -------------------------------\n",
    "# 6. (Optional) Sample Q/A Predictions (Qualitative)\n",
    "# -------------------------------\n",
    "def test_model_predictions(model_dir, samples):\n",
    "    print(f\"--- Sample QA from Model: {model_dir} ---\")\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(model_dir)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_dir)\n",
    "    qa_pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=\"cuda\" if torch.cuda.is_available() else -1)\n",
    "\n",
    "    for i, example in enumerate(samples):  # Correct: loop over rows directly\n",
    "        question = example[\"question\"]\n",
    "        context = example[\"context\"]\n",
    "        ta = example[\"answers\"][\"text\"] # Ground truth answer\n",
    "        result = qa_pipe(question=question, context=context)\n",
    "        print(f\"Q{i + 1}: {question}\")\n",
    "        print(f\"C{i + 1}: {context[:200]}...\")\n",
    "        print(f\"A{i + 1}: {result['answer']}\")\n",
    "        print(f\"GTA{i + 1}: {ta[:100]}\\n\")  # Show first 100 chars of ground truth answer\n",
    "\n",
    "print(\"ğŸ§ª Running Sample QA Predictions...\")\n",
    "samples = datasets_ori[\"validation\"].select(range(3))  # Get first 3 examples\n",
    "test_model_predictions(model_name_base, samples)   # Base\n",
    "test_model_predictions(first_model_dir, samples)    # First fine-tuned\n",
    "test_model_predictions(second_model_dir, samples)   # Second fine-tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0181b-0d00-4cb2-9826-705de9db991f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
