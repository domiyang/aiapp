{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c219841f-493c-40f9-a6c9-3700f0c525d0",
   "metadata": {},
   "source": [
    "# PEFT with LoRA using OpenAI Whisper-large-v2\n",
    "\n",
    "```\n",
    "Á¨¨‰∫åÂë®‰Ωú‰∏ö‰∏Ä:\n",
    "1„ÄÅ‰ΩøÁî®ÂÆåÊï¥ÁöÑ YelpReviewFull Êï∞ÊçÆÈõÜËÆ≠ÁªÉÔºåÂØπÊØîÁúã Acc ÊúÄÈ´òËÉΩÂà∞Â§öÂ∞ë„ÄÇËØæÁ®ã‰ª£Á†ÅÔºà https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-quickstart.ipynb Ôºâ\n",
    "2„ÄÅÂä†ËΩΩÊú¨Âú∞‰øùÂ≠òÁöÑÊ®°ÂûãÔºåËøõË°åËØÑ‰º∞ÂíåÂÜçËÆ≠ÁªÉÊõ¥È´òÁöÑ F1 Score„ÄÇËØæÁ®ã‰ª£Á†ÅÔºà https://github.com/DjangoPeng/LLM-quickstart/blob/main/transformers/fine-tune-QA.ipynb Ôºâ\n",
    "\n",
    "Á¨¨‰∫åÂë®‰Ωú‰∏ö‰∫å: \n",
    "1„ÄÅÂú®‚ÄúLoRA ‰ΩéÁß©ÈÄÇÈÖç OpenAI Whisper-Large-V2 ËØ≠Èü≥ËØÜÂà´‰ªªÂä°‚Äù‰∏≠Ôºå‰∏∫‰∏≠ÊñáËØ≠ÊñôÁöÑËÆ≠ÁªÉËøáÁ®ãÂ¢ûÂä†ËøáÁ®ãËØÑ‰º∞ÔºåËßÇÂØü Train Loss Âíå Validation Loss ÂèòÂåñ„ÄÇËØæÁ®ã‰ª£Á†ÅÔºà https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_lora_whisper-large-v2.ipynb Ôºâ------> this notebook is for this task. \n",
    "2„ÄÅÂú®‚ÄúLoRA ‰ΩéÁß©ÈÄÇÈÖç OpenAI Whisper-Large-V2 ËØ≠Èü≥ËØÜÂà´‰ªªÂä°‚Äù‰∏≠ÔºåÂΩì LoRA Ê®°ÂûãËÆ≠ÁªÉÂÆåÊàêÂêéÔºå‰ΩøÁî®ÊµãËØïÈõÜËøõË°åÂÆåÊï¥ÁöÑÊ®°ÂûãËØÑ‰º∞„ÄÇËØæÁ®ã‰ª£Á†ÅÔºà https://github.com/DjangoPeng/LLM-quickstart/blob/main/peft/peft_lora_whisper-large-v2.ipynb Ôºâ ------> this notebook is \n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd00402-d821-485e-8703-fb16bcb56a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Tokenizing SMALL dataset: 640 train, 320 val\n",
      "üöÄ Tokenizing SMALL dataset with NUM_PROC=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c080c657bf46ff8c9fd63629831e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18783b9666dc4b51bf5fc3545adf3669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================\n",
    "# 1. IMPORTS\n",
    "# =============================================\n",
    "\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "# =============================================\n",
    "# 2. GLOBAL CONFIGS / CONSTANTS\n",
    "# =============================================\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"openai/whisper-large-v2\"\n",
    "MODEL_DIR_BASE = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "LANGUAGE = \"Chinese (China)\"\n",
    "LANGUAGE_ABBR = \"zh-CN\"\n",
    "TASK = \"transcribe\"\n",
    "\n",
    "DATASET_NAME = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "LOGGING_STEPS = 1\n",
    "\n",
    "# for faster process .map (e.g.: 8 cpus then use 2, too high might freez the system)\n",
    "NUM_PROC = 2\n",
    "\n",
    "# Small dataset sizes for quick iteration\n",
    "SMALL_TRAIN_SIZE = 640\n",
    "SMALL_VAL_SIZE = 320\n",
    "\n",
    "# =============================================\n",
    "# 3. LOAD DATASET\n",
    "# =============================================\n",
    "\n",
    "common_voice = DatasetDict({\n",
    "    \"train\": load_dataset(DATASET_NAME, LANGUAGE_ABBR, split=\"train\", trust_remote_code=True),\n",
    "    \"validation\": load_dataset(DATASET_NAME, LANGUAGE_ABBR, split=\"validation\", trust_remote_code=True),\n",
    "})\n",
    "\n",
    "# =============================================\n",
    "# 4. PREPROCESS: REMOVE UNUSED COLUMNS, RESAMPLE AUDIO\n",
    "# =============================================\n",
    "\n",
    "columns_to_remove = [\n",
    "    \"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\",\n",
    "    \"locale\", \"path\", \"segment\", \"up_votes\"\n",
    "]\n",
    "\n",
    "def preprocess_dataset(ds):\n",
    "    ds = ds.remove_columns(columns_to_remove)\n",
    "    ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    return ds\n",
    "\n",
    "common_voice = DatasetDict({\n",
    "    \"train\": preprocess_dataset(common_voice[\"train\"]),\n",
    "    \"validation\": preprocess_dataset(common_voice[\"validation\"]),\n",
    "})\n",
    "\n",
    "# =============================================\n",
    "# 5. LOAD MODEL & PROCESSOR COMPONENTS\n",
    "# =============================================\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH, language=LANGUAGE, task=TASK)\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME_OR_PATH, language=LANGUAGE, task=TASK)\n",
    "\n",
    "# =============================================\n",
    "# 6. PREPARE DATASET FUNCTION\n",
    "# =============================================\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "    ).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# =============================================\n",
    "# 7. DEFINE DATA COLLATOR\n",
    "# =============================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# =============================================\n",
    "# 8. LOAD BASE MODEL (8-bit) & APPLY PEFT (LoRA)\n",
    "# =============================================\n",
    "\n",
    "def load_peft_model():\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        MODEL_NAME_OR_PATH,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    # prepare model for int8 \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=64,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    peft_model.config.use_cache = False\n",
    "    return peft_model\n",
    "\n",
    "# =============================================\n",
    "# 9. TRAINING FUNCTION (Reusable)\n",
    "# =============================================\n",
    "\n",
    "def run_training_run(name: str, train_ds, eval_ds, output_dir_suffix: str = \"\"):\n",
    "    print(f\"üîÅ Starting training run: {name}\")\n",
    "    model_dir = f\"{MODEL_DIR_BASE}\"\n",
    "    \n",
    "    peft_model = load_peft_model()\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=model_dir,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        generation_max_length=128\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=peft_model,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    print(f\"Training run '{name}' started...\")\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_dir)\n",
    "    print(f\"‚úÖ Training run '{name}' completed and model saved to {model_dir}\")\n",
    "    peft_model.eval()\n",
    "    print(f\"started evaluating...\")\n",
    "    # Return final metrics (e.g., eval loss)\n",
    "    metrics = trainer.evaluate()\n",
    "    print(f\"{name} - metrics: {metrics}\")\n",
    "    print(f\"{name} - Final Eval Loss: {metrics.get('eval_loss', 'N/A'):.4f}\")\n",
    "    return metrics\n",
    "\n",
    "# =============================================\n",
    "# 10. DATASET VARIANTS (Optimized)\n",
    "# =============================================\n",
    "\n",
    "print(f\"üîπ Tokenizing SMALL dataset: {SMALL_TRAIN_SIZE} train, {SMALL_VAL_SIZE} val\")\n",
    "\n",
    "# 1. Shuffle and select SMALL subsets from the ORIGINAL (raw) dataset first\n",
    "small_train_raw = common_voice[\"train\"].shuffle(seed=16).select(range(SMALL_TRAIN_SIZE))\n",
    "small_val_raw = common_voice[\"validation\"].shuffle(seed=16).select(range(SMALL_VAL_SIZE))\n",
    "\n",
    "# 2. Tokenize ONLY the small subset (much faster!)\n",
    "print(f\"üöÄ Tokenizing SMALL dataset with NUM_PROC={NUM_PROC}\")\n",
    "small_tokenized = DatasetDict({\n",
    "    \"train\": small_train_raw.map(prepare_dataset, num_proc=NUM_PROC),\n",
    "    \"validation\": small_val_raw.map(prepare_dataset, num_proc=NUM_PROC),\n",
    "})\n",
    "\n",
    "# Extract small train/val after tokenization\n",
    "small_train = small_tokenized[\"train\"]\n",
    "small_val = small_tokenized[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1e3668-6f4a-430d-8401-a8d5c6fd6853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Starting training run: small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/peft/utils/other.py:141: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.12723204856023188\n",
      "Training run 'small' started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 09:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.935900</td>\n",
       "      <td>1.081708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training run 'small' completed and model saved to models/whisper-large-v2-asr-int8\n",
      "started evaluating...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 02:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small - metrics: {'eval_loss': 1.0817080736160278, 'eval_runtime': 185.2453, 'eval_samples_per_second': 1.727, 'eval_steps_per_second': 0.027, 'epoch': 1.0}\n",
      "small - Final Eval Loss: 1.0817\n",
      "üîπ Small Dataset - Eval Loss: 1.0817\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 11. RUN the training\n",
    "# =============================================\n",
    "# Train on SMALL dataset\n",
    "small_metrics = run_training_run(\n",
    "    name=\"small\",\n",
    "    train_ds=small_train,\n",
    "    eval_ds=small_val,\n",
    ")\n",
    "print(f\"üîπ Small Dataset - Eval Loss: {small_metrics.get('eval_loss', 'N/A'):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2890f5-2eb9-493d-b43b-266fb12c6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: you'll need to restart the kernel and run this cell or will got error: ValueError\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üîç TRANSCRIPTION COMPARISON\n",
      "==================================================\n",
      "üü¢ [Base Model] Whisper (Original, 8-bit):\n",
      " This is a test for the automatic voice recognition of the WhisperLine Large V2 model.\n",
      "üîµ [PEFT Model] Fine-Tuned Model:\n",
      "ËøôÊòØ‰∏ÄÊÆµÊµãËØïÁî®‰∫éWhisperLarge V2Ê®°ÂûãÁöÑËá™Âä®ËØ≠Èü≥ËØÜÂà´ÊµãËØï„ÄÇ\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"NOTE: you'll need to restart the kernel and run this cell or will got error: ValueError\")\n",
    "# =============================================\n",
    "# 0. IMPORTS (All imports at the top ‚Äî PEP 8 compliant)\n",
    "# =============================================\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    ")\n",
    "from peft import PeftConfig, PeftModel\n",
    "import torch\n",
    "\n",
    "# =============================================\n",
    "# 1. CONFIGURATION / CONSTANTS\n",
    "# =============================================\n",
    "\n",
    "# --- Model & Language Settings ---\n",
    "MODEL_DIR = \"models/whisper-large-v2-asr-int8\"  # path to the fine-tuned PEFT model\n",
    "BASE_MODEL_NAME_OR_PATH = \"openai/whisper-large-v2\"  # original base Whisper model\n",
    "LANGUAGE = \"Chinese (China)\"\n",
    "LANGUAGE_ABBR = \"zh-CN\"         # language code used during training\n",
    "LANGUAGE_DECODE = \"chinese\"     # used for forced decoder IDs\n",
    "TASK = \"transcribe\"             # task type\n",
    "\n",
    "# --- Test Audio File ---\n",
    "TEST_AUDIO_PATH = \"data/audio/test_zh.flac\"\n",
    "\n",
    "# =============================================\n",
    "# 2. LOAD BASE MODEL (ORIGINAL WHISPER, 8-BIT)\n",
    "# =============================================\n",
    "\n",
    "# Load the base Whisper model (8-bit quantized)\n",
    "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    BASE_MODEL_NAME_OR_PATH,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Load tokenizer, processor, and feature extractor using the base model's original config\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_NAME_OR_PATH,\n",
    "    language=LANGUAGE_ABBR,\n",
    "    task=TASK,\n",
    ")\n",
    "processor_base = AutoProcessor.from_pretrained(\n",
    "    BASE_MODEL_NAME_OR_PATH,\n",
    "    language=LANGUAGE_ABBR,\n",
    "    task=TASK,\n",
    ")\n",
    "feature_extractor_base = processor_base.feature_extractor\n",
    "\n",
    "# =============================================\n",
    "# 3. LOAD PEFT MODEL (FINETUNED ON TOP OF BASE)\n",
    "# =============================================\n",
    "\n",
    "# Load the PEFT config to get the base model path (should match BASE_MODEL_NAME_OR_PATH)\n",
    "peft_config = PeftConfig.from_pretrained(MODEL_DIR)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        peft_config.base_model_name_or_path,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    ),\n",
    "    MODEL_DIR,\n",
    ")\n",
    "\n",
    "# Load tokenizer, processor, and feature extractor (same as training)\n",
    "tokenizer_peft = AutoTokenizer.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    language=LANGUAGE_ABBR,\n",
    "    task=TASK,\n",
    ")\n",
    "processor_peft = AutoProcessor.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    language=LANGUAGE_ABBR,\n",
    "    task=TASK,\n",
    ")\n",
    "feature_extractor_peft = processor_peft.feature_extractor\n",
    "\n",
    "# =============================================\n",
    "# 4. CREATE TWO ASR PIPELINES (Base vs PEFT)\n",
    "# =============================================\n",
    "\n",
    "# --- Base Model Pipeline ---\n",
    "pipeline_base = AutomaticSpeechRecognitionPipeline(\n",
    "    model=base_model,\n",
    "    tokenizer=tokenizer_base,\n",
    "    feature_extractor=feature_extractor_base,\n",
    ")\n",
    "\n",
    "# --- PEFT Model Pipeline ---\n",
    "pipeline_peft = AutomaticSpeechRecognitionPipeline(\n",
    "    model=peft_model,\n",
    "    tokenizer=tokenizer_peft,\n",
    "    feature_extractor=feature_extractor_peft,\n",
    ")\n",
    "\n",
    "# =============================================\n",
    "# 5. (OPTIONAL) FORCED DECODER IDS (for Chinese)\n",
    "# =============================================\n",
    "\n",
    "forced_decoder_ids_base = processor_base.get_decoder_prompt_ids(language=LANGUAGE_DECODE, task=TASK)\n",
    "forced_decoder_ids_peft = processor_peft.get_decoder_prompt_ids(language=LANGUAGE_DECODE, task=TASK)\n",
    "\n",
    "# Note: Most of the time, the pipeline will use the correct tokenizer/language settings\n",
    "# automatically. These are here for advanced manual control if needed.\n",
    "\n",
    "# =============================================\n",
    "# 6. RUN INFERENCE ON BOTH MODELS (Same Audio)\n",
    "# =============================================\n",
    "\n",
    "# --- Base Model Inference ---\n",
    "with torch.cuda.amp.autocast():\n",
    "    result_base = pipeline_base(TEST_AUDIO_PATH, max_new_tokens=255)\n",
    "    transcription_base = result_base[\"text\"]\n",
    "\n",
    "# --- PEFT Model Inference ---\n",
    "with torch.cuda.amp.autocast():\n",
    "    result_peft = pipeline_peft(TEST_AUDIO_PATH, max_new_tokens=255)\n",
    "    transcription_peft = result_peft[\"text\"]\n",
    "\n",
    "# =============================================\n",
    "# 7. OUTPUT COMPARISON\n",
    "# =============================================\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"üîç TRANSCRIPTION COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üü¢ [Base Model] Whisper (Original, 8-bit):\")\n",
    "print(transcription_base)\n",
    "\n",
    "print(\"üîµ [PEFT Model] Fine-Tuned Model:\")\n",
    "print(transcription_peft)\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d024260d-7b9d-4bac-8873-1bf424342b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
