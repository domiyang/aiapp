{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0db63f-8d18-4929-9535-edf58ae15e62",
   "metadata": {},
   "source": [
    "# llama2 finetune and inference with Alpaca-Style instructions\n",
    "\n",
    "```\n",
    "ç¬¬äº”å‘¨ä½œä¸šï¼š\n",
    "ä½¿ç”¨è¯¾ç¨‹ä¸­çš„æ•°æ®é›†å’Œä»£ç ï¼Œå®Œæˆ LLaMA æŒ‡ä»¤å¾®è°ƒè®­ç»ƒ\n",
    "ä½œä¸šä»£ç ï¼š\n",
    "https://github.com/DjangoPeng/LLM-quickstart/tree/main/llama ------> this notebook is for this task.\n",
    "\n",
    "å­¦ä¹ é‡ç‚¹ï¼š\n",
    "å­¦ä¹ å¦‚ä½•LLaMA 2 æŒ‡ä»¤å¾®è°ƒï¼Œæ€ä¹ˆä»¥ Alpaca-Style æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®ã€‚\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97de889-4d1d-4ee3-ad57-289e2e5919d8",
   "metadata": {},
   "source": [
    "### ä¸‹è½½ databricks-dolly-15k æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef235f0d-df0d-4be9-a034-af6684588dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    " \n",
    "# ä»hubåŠ è½½æ•°æ®é›†\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c5f398a-6dd4-4f71-a474-1b647103471a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'context', 'response', 'category'],\n",
       "    num_rows: 15011\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°æ®é›†æ ·ä¾‹æ€»æ•°: 15011\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61788bc1-6c19-4301-9089-14a59c23b869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'In the series A Song of Ice and Fire, who is the founder of House Bulwer?', 'context': '', 'response': 'Bors the Breaker', 'category': 'open_qa'}\n"
     ]
    }
   ],
   "source": [
    "# éšæœºæŠ½é€‰ä¸€ä¸ªæ•°æ®æ ·ä¾‹æ‰“å°\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71040ca1-92d0-4e1e-af66-49ef2a52c3d7",
   "metadata": {},
   "source": [
    "### ä»¥ Alpaca-Style æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®\n",
    "\n",
    "`Alpacca-style` æ ¼å¼ï¼šhttps://github.com/tatsu-lab/stanford_alpaca#data-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d2889f7-ff56-4ee5-a852-1794c0a296ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample_data):\n",
    "    \"\"\"\n",
    "    Formats the given data into a structured instruction format.\n",
    "\n",
    "    Parameters:\n",
    "    sample_data (dict): A dictionary containing 'response' and 'instruction' keys.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted string containing the instruction, input, and response.\n",
    "    \"\"\"\n",
    "    # Check if required keys exist in the sample_data\n",
    "    if 'response' not in sample_data or 'instruction' not in sample_data:\n",
    "        # Handle the error or return a default message\n",
    "        return \"Error: 'response' or 'instruction' key missing in the input data.\"\n",
    "\n",
    "    return f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    " \n",
    "### Input:\n",
    "{sample_data['response']}\n",
    " \n",
    "### Response:\n",
    "{sample_data['instruction']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26167b60-f339-487a-a2dc-0cf49fac8932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
      "\n",
      "### Input:\n",
      "Apache Flink is an open-source, unified stream-processing and batch-processing framework developed by the Apache Software Foundation. The core of Apache Flink is a distributed streaming data-flow engine written in Java and Scala. Flink executes arbitrary dataflow programs in a data-parallel and pipelined manner.\n",
      "\n",
      "### Response:\n",
      "What is Flink?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# éšæœºæŠ½é€‰ä¸€ä¸ªæ ·ä¾‹ï¼Œæ‰“å° Alpaca æ ¼å¼åŒ–åçš„æ ·ä¾‹ \n",
    "print(format_instruction(dataset[randrange(len(dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b77d2-58e2-4018-a943-51bbfd68ee4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eea57c8-ffac-422c-9fa1-b4b98dd3f917",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨å¿«é€Ÿæ³¨æ„åŠ›ï¼ˆFlash Attentionï¼‰åŠ é€Ÿè®­ç»ƒ\n",
    "\n",
    "æ£€æŸ¥ä½ çš„ GPU æ˜¯å¦æ”¯æŒ `flash-attn` åŠ é€Ÿï¼š\n",
    "\n",
    "```shell\n",
    "$ python -c \"import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\"\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"<string>\", line 1, in <module>\n",
    "AssertionError: Hardware not supported for Flash Attention\n",
    "```\n",
    "**è¿è¡Œç»“æœï¼šæ¼”ç¤ºä½¿ç”¨çš„ NVIDIA T4 ç¡¬ä»¶ä¸æ”¯æŒ Flash Attention**\n",
    "\n",
    "#### å®‰è£… flash-attn åŠ é€ŸåŒ…ï¼ˆéœ€è¦GPUç¡¬ä»¶æ”¯æŒï¼‰\n",
    "\n",
    "```shell\n",
    "$ MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a715dc-3ecd-41bb-9b2e-88c1db991e92",
   "metadata": {},
   "source": [
    "### åŠ è½½æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677fa03d-439e-4aa1-81f0-e1015d71c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7fab6d081c4623b7e7aa31959c97bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# å¦‚æœç¡¬ä»¶è®¾å¤‡æ”¯æŒï¼ŒæˆåŠŸå®‰è£… flash-attnåï¼Œå°† use_flash_attention è®¾ç½®ä¸ºTrue\n",
    "# T4 doesn't support, so keep as false\n",
    "use_flash_attention = False\n",
    " \n",
    "# å–æ¶ˆæ³¨é‡Šä»¥ä½¿ç”¨ flash-atten\n",
    "# if torch.cuda.get_device_capability()[0] >= 8:\n",
    "#     from utils.llama_patch import replace_attn_with_flash_attn\n",
    "#     print(\"Using flash attention\")\n",
    "#     replace_attn_with_flash_attn()\n",
    "#     use_flash_attention = True\n",
    " \n",
    " \n",
    "# è·å– LLaMA 2-7B æ¨¡å‹æƒé‡\n",
    "# æ— éœ€ Meta AI å®¡æ ¸çš„æ¨¡å‹æƒé‡\n",
    "model_id = \"NousResearch/Llama-2-7b-hf\" \n",
    "# é€šè¿‡ Meta AI å®¡æ ¸åå¯ä½¿ç”¨æ­¤ Model ID ä¸‹è½½\n",
    "# model_id = \"meta-llama/Llama-2-7b-hf\" \n",
    " \n",
    " \n",
    "# ä½¿ç”¨ BnB åŠ è½½é‡åŒ–åçš„æ¨¡å‹\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    " \n",
    "# åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, use_cache=False, device_map=\"auto\")\n",
    "model.config.pretraining_tp = 1 \n",
    " \n",
    "# é€šè¿‡å¯¹æ¯”docä¸­çš„å­—ç¬¦ä¸²ï¼ŒéªŒè¯æ¨¡å‹æ˜¯å¦åœ¨ä½¿ç”¨flash attention\n",
    "if use_flash_attention:\n",
    "    from utils.llama_patch import forward    \n",
    "    assert model.model.layers[0].self_attn.forward.__doc__ == forward.__doc__, \"Model is not using flash attention\"\n",
    " \n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b1b42-97f2-4378-99d3-d582fae48e03",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ QLoRA é…ç½®åŠ è½½ PEFT æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50690552-1807-4e53-892a-1aad10cadaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    " \n",
    "# QLoRA é…ç½®\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=16,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    ")\n",
    " \n",
    " \n",
    "# ä½¿ç”¨ QLoRA é…ç½®åŠ è½½ PEFT æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "qlora_model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98490c6a-fa9d-460b-8362-33444efd2e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.12433454005023165\n"
     ]
    }
   ],
   "source": [
    "qlora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0f7b6-30a3-4e10-8a4a-9840e465e590",
   "metadata": {},
   "source": [
    "### è®­ç»ƒè¶…å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb427d32-575f-4af7-af71-bbcfc0b3b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output directory: models/llama-7-int4-dolly-20250824_130335\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# æ¼”ç¤ºè®­ç»ƒå‚æ•°ï¼ˆå®é™…è®­ç»ƒæ˜¯è®¾ç½®ä¸º Falseï¼‰\n",
    "demo_train = True\n",
    "output_dir = f\"models/llama-7-int4-dolly-{timestamp}\"\n",
    "\n",
    "print(f\"Model output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f57027d-63cc-4199-9ae7-d04fa9db3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1 if demo_train else 3,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=3, # Nvidia T4 16GB æ˜¾å­˜æ”¯æŒçš„æœ€å¤§ Batch Size\n",
    "    gradient_accumulation_steps=1 if demo_train else 4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\" if demo_train else \"epoch\",\n",
    "    save_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e28e91-6daa-44c9-8e1e-59abddd57697",
   "metadata": {},
   "source": [
    "### å®ä¾‹åŒ– SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c29ebe-f962-4b2f-9905-743f63024cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:341: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    " \n",
    "# æ•°æ®é›†çš„æœ€å¤§é•¿åº¦åºåˆ—ï¼ˆç­›é€‰åçš„è®­ç»ƒæ•°æ®æ ·ä¾‹æ•°ä¸º1158ï¼‰\n",
    "max_seq_length = 2048 \n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=qlora_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    formatting_func=format_instruction, \n",
    "    args=args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716498f-93c7-4071-858e-c53d270eeb2d",
   "metadata": {},
   "source": [
    "### è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43d07ef2-d07c-41c1-8eaa-2472f289055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model, output directory: models/llama-7-int4-dolly-20250824_130335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 3:25:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.221800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.184500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.262000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.285500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.295439977645874, metrics={'train_runtime': 12450.4755, 'train_samples_per_second': 0.024, 'train_steps_per_second': 0.008, 'total_flos': 2.43882352705536e+16, 'train_loss': 1.295439977645874, 'epoch': 0.26})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Training model, output directory: {output_dir}\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5eeb5e-2082-4ead-b4a2-ebedb06333f9",
   "metadata": {},
   "source": [
    "### ä¿å­˜æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e874176b-1b1a-4264-b24e-5308c773e81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/llama-7-int4-dolly-20250824_130335\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39256f7-6b10-4f6d-8a99-c96a512604ec",
   "metadata": {},
   "source": [
    "### æ¨¡å‹æ¨ç†ï¼ˆæµ‹è¯•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e38d38d-a675-40a1-aa0d-b78e90865162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”§ Releasing GPU ...\n",
      "Loading model from models/llama-7-int4-dolly-20250824_130335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/byenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6122a85db6654e77be364fb9d1a97b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "Alexis de Tocqueville wrote Democracy in America\n",
      "\n",
      "Generated instruction:\n",
      "From whom did Alexis de Tocqueville receive inspiration when writing Democracy in America?\n",
      "\n",
      "Ground truth:\n",
      "Who wrote Democracy in America?\n"
     ]
    }
   ],
   "source": [
    "# this can run multiple times\n",
    "# function to free up mem and remove all variables\n",
    "def freeup_mem_deep():\n",
    "    print(\"\\nğŸ”§ Releasing GPU ...\")\n",
    "    import torch, gc\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    #%reset -f\n",
    "# call to freeup mem    \n",
    "freeup_mem_deep()\n",
    "\n",
    "import torch\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset \n",
    "from random import randrange\n",
    "\n",
    "# specify the timestamp if needed\n",
    "timestamp = \"20250824_130335\"\n",
    "model_dir = f\"models/llama-7-int4-dolly-{timestamp}\"\n",
    "print(f\"Loading model from {model_dir}\")\n",
    " \n",
    "# åŠ è½½åŸºç¡€LLMæ¨¡å‹ä¸åˆ†è¯å™¨\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ") \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "\n",
    " \n",
    "# ä»hubåŠ è½½æ•°æ®é›†å¹¶å¾—åˆ°ä¸€ä¸ªæ ·æœ¬\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "sample = dataset[randrange(len(dataset))]\n",
    " \n",
    "prompt = f\"\"\"### Instruction:\n",
    "Use the Input below to create an instruction, which could have been used to generate the input using an LLM. \n",
    " \n",
    "### Input:\n",
    "{sample['response']}\n",
    " \n",
    "### Response:\n",
    "\"\"\"\n",
    " \n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "outputs = model.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_p=0.9,temperature=0.9)\n",
    "\n",
    "print(f\"Prompt:\\n{sample['response']}\\n\")\n",
    "print(f\"Generated instruction:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0][len(prompt):]}\")\n",
    "print(f\"Ground truth:\\n{sample['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db2e4f-c724-4c17-9ad3-e1190f6fa75a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "07de48088dc14163935202192b8649e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1b9017a8f6fa4b7c912abc6f93907614": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "21123ccb6aa945a6b8f76adf4f399eaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "24e59ecb397c4733a5cbbe83f32afeab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ec23ab62be814188bb548f4624d56edc",
       "style": "IPY_MODEL_21123ccb6aa945a6b8f76adf4f399eaa",
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "2a8936e2ea734d829f85af5660be2d21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_7e8fd355f6214f0fb1ec5ea3d405e25d",
       "style": "IPY_MODEL_9e23151103b54e4bba38053bd88e2192",
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "349fbdd53148415abc0a1af3a6458ead": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "35477f092023427eacf18f33de2e1881": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_f8d6f2d2949e4b0eabb82ec641d5ded4",
       "max": 2,
       "style": "IPY_MODEL_3bc45e1f7da94f159205a60687688264",
       "value": 2
      }
     },
     "3bc45e1f7da94f159205a60687688264": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "42130213fbed4969ba6fd84cd370df71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4222493c86e446ee9fc847c862eaff03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "426dd0c6b47344299c62868bfd014400": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5534b835344f47ddb48e556a44c3db4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_755ffe5badf34ea094d975903e519a3d",
       "style": "IPY_MODEL_a70c84cc144441d885d21a0be33549cb",
       "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡2.09s/it]"
      }
     },
     "5fe1f8d7200643c9869dd03ce1450b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6122a85db6654e77be364fb9d1a97b4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2a8936e2ea734d829f85af5660be2d21",
        "IPY_MODEL_d8f765ee4bb54b70a7a4e555d195c765",
        "IPY_MODEL_b9a20aeba4224bceb4b3029dff049906"
       ],
       "layout": "IPY_MODEL_6db3cb60c2b148e18ce409181f92895f"
      }
     },
     "6208af029fe742feabe34646816e0770": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_07de48088dc14163935202192b8649e1",
       "style": "IPY_MODEL_cf2d574ffe184908b4bf31ae4245e086",
       "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
      }
     },
     "6db3cb60c2b148e18ce409181f92895f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6e95fce7d7cf4821ac7c307529646eca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_349fbdd53148415abc0a1af3a6458ead",
       "style": "IPY_MODEL_5fe1f8d7200643c9869dd03ce1450b21",
       "value": "â€‡2/2â€‡[00:04&lt;00:00,â€‡â€‡2.15s/it]"
      }
     },
     "755ffe5badf34ea094d975903e519a3d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7d7fab6d081c4623b7e7aa31959c97bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_6208af029fe742feabe34646816e0770",
        "IPY_MODEL_ca815514c1a646c8aacc7226a2f93afa",
        "IPY_MODEL_5534b835344f47ddb48e556a44c3db4e"
       ],
       "layout": "IPY_MODEL_cf92bb334bff43489bcd45eb5ab6392a"
      }
     },
     "7e8fd355f6214f0fb1ec5ea3d405e25d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8178b3f7ac9e4980acd7ad9c8a1edea0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "91f7e502ee7545bc9a0140dc6501bd09": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e23151103b54e4bba38053bd88e2192": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a70c84cc144441d885d21a0be33549cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9a20aeba4224bceb4b3029dff049906": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_91f7e502ee7545bc9a0140dc6501bd09",
       "style": "IPY_MODEL_1b9017a8f6fa4b7c912abc6f93907614",
       "value": "â€‡2/2â€‡[00:05&lt;00:00,â€‡â€‡2.32s/it]"
      }
     },
     "c1cf1fc7a0b84de4a505424aabce5165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_24e59ecb397c4733a5cbbe83f32afeab",
        "IPY_MODEL_35477f092023427eacf18f33de2e1881",
        "IPY_MODEL_6e95fce7d7cf4821ac7c307529646eca"
       ],
       "layout": "IPY_MODEL_4222493c86e446ee9fc847c862eaff03"
      }
     },
     "ca815514c1a646c8aacc7226a2f93afa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_426dd0c6b47344299c62868bfd014400",
       "max": 2,
       "style": "IPY_MODEL_ea7381fb3f43438a8732486ad0d9a34b",
       "value": 2
      }
     },
     "cf2d574ffe184908b4bf31ae4245e086": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cf92bb334bff43489bcd45eb5ab6392a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8f765ee4bb54b70a7a4e555d195c765": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_42130213fbed4969ba6fd84cd370df71",
       "max": 2,
       "style": "IPY_MODEL_8178b3f7ac9e4980acd7ad9c8a1edea0",
       "value": 2
      }
     },
     "ea7381fb3f43438a8732486ad0d9a34b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ec23ab62be814188bb548f4624d56edc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f8d6f2d2949e4b0eabb82ec641d5ded4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
